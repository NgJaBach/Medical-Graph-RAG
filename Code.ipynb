{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a57e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from pydantic import BaseModel\n",
    "from langchain import hub\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from camel.storages import Neo4jGraph\n",
    "from camel.agents import KnowledgeGraphAgent\n",
    "from camel.loaders import UnstructuredIO\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tiktoken\n",
    "import uuid\n",
    "from typing import Optional, List\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6db1cc",
   "metadata": {},
   "source": [
    "# Summerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299790f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "sum_prompt = \"\"\"\n",
    "Generate a structured summary from the provided medical source (report, paper, or book), strictly adhering to the following categories. The summary should list key information under each category in a concise format: 'CATEGORY_NAME: Key information'. No additional explanations or detailed descriptions are necessary unless directly related to the categories:\n",
    "\n",
    "ANATOMICAL_STRUCTURE: Mention any anatomical structures specifically discussed.\n",
    "BODY_FUNCTION: List any body functions highlighted.\n",
    "BODY_MEASUREMENT: Include normal measurements like blood pressure or temperature.\n",
    "BM_RESULT: Results of these measurements.\n",
    "BM_UNIT: Units for each measurement.\n",
    "BM_VALUE: Values of these measurements.\n",
    "LABORATORY_DATA: Outline any laboratory tests mentioned.\n",
    "LAB_RESULT: Outcomes of these tests (e.g., 'increased', 'decreased').\n",
    "LAB_VALUE: Specific values from the tests.\n",
    "LAB_UNIT: Units of measurement for these values.\n",
    "MEDICINE: Name medications discussed.\n",
    "MED_DOSE, MED_DURATION, MED_FORM, MED_FREQUENCY, MED_ROUTE, MED_STATUS, MED_STRENGTH, MED_UNIT, MED_TOTALDOSE: Provide concise details for each medication attribute.\n",
    "PROBLEM: Identify any medical conditions or findings.\n",
    "PROCEDURE: Describe any procedures.\n",
    "PROCEDURE_RESULT: Outcomes of these procedures.\n",
    "PROC_METHOD: Methods used.\n",
    "SEVERITY: Severity of the conditions mentioned.\n",
    "MEDICAL_DEVICE: List any medical devices used.\n",
    "SUBSTANCE_ABUSE: Note any substance abuse mentioned.\n",
    "Each category should be addressed only if relevant to the content of the medical source. Ensure the summary is clear and direct, suitable for quick reference.\n",
    "\"\"\"\n",
    "\n",
    "# Add your own OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = \"gpt-4.1-nano\"\n",
    "embedding = \"text-embedding-3-small\"\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_gpt(chunk) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sum_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\" {chunk}\"},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def split_into_chunks(text, tokens=500):\n",
    "    encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "    words = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), tokens):\n",
    "        chunks.append(' '.join(encoding.decode(words[i:i + tokens])))\n",
    "    return chunks   \n",
    "\n",
    "def process_chunks(content):\n",
    "    chunks = split_into_chunks(content)\n",
    "\n",
    "    # Processes chunks in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        responses = list(executor.map(ask_gpt, chunks))\n",
    "    # print(responses)\n",
    "    return responses\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    content = \" sth you wanna test\"\n",
    "    process_chunks(content)\n",
    "\n",
    "# Can take up to a few minutes to run depending on the size of your data input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0f616",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6fba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt_one = \"\"\"\n",
    "Please answer the question using insights supported by provided graph-based data relevant to medical information.\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_two = \"\"\"\n",
    "Modify the response to the question using the provided references. Include precise citations relevant to your answer. You may use multiple citations simultaneously, denoting each with the reference index number. For example, cite the first and third documents as [1][3]. If the references do not pertain to the response, simply provide a concise answer to the original question.\n",
    "\"\"\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model= embedding\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def fetch_texts(n4j):\n",
    "    # Fetch the text for each node\n",
    "    query = \"MATCH (n) RETURN n.id AS id\"\n",
    "    return n4j.query(query)\n",
    "\n",
    "def add_embeddings(n4j, node_id, embedding):\n",
    "    # Upload embeddings to Neo4j\n",
    "    query = \"MATCH (n) WHERE n.id = $node_id SET n.embedding = $embedding\"\n",
    "    n4j.query(query, params = {\"node_id\":node_id, \"embedding\":embedding})\n",
    "\n",
    "def add_nodes_emb(n4j):\n",
    "    nodes = fetch_texts(n4j)\n",
    "\n",
    "    for node in nodes:\n",
    "        # Calculate embedding for each node's text\n",
    "        if node['id']:  # Ensure there is text to process\n",
    "            embedding = get_embedding(node['id'])\n",
    "            # Store embedding back in the node\n",
    "            add_embeddings(n4j, node['id'], embedding)\n",
    "\n",
    "def add_ge_emb(graph_element):\n",
    "    for node in graph_element.nodes:\n",
    "        emb = get_embedding(node.id)\n",
    "        node.properties['embedding'] = emb\n",
    "    return graph_element\n",
    "\n",
    "def add_gid(graph_element, gid):\n",
    "    for node in graph_element.nodes:\n",
    "        node.properties['gid'] = gid\n",
    "    for rel in graph_element.relationships:\n",
    "        rel.properties['gid'] = gid\n",
    "    return graph_element\n",
    "\n",
    "def add_sum(n4j,content,gid):\n",
    "    sum = process_chunks(content)\n",
    "    creat_sum_query = \"\"\"\n",
    "        CREATE (s:Summary {content: $sum, gid: $gid})\n",
    "        RETURN s\n",
    "        \"\"\"\n",
    "    s = n4j.query(creat_sum_query, {'sum': sum, 'gid': gid})\n",
    "    \n",
    "    link_sum_query = \"\"\"\n",
    "        MATCH (s:Summary {gid: $gid}), (n)\n",
    "        WHERE n.gid = s.gid AND NOT n:Summary\n",
    "        CREATE (s)-[:SUMMARIZES]->(n)\n",
    "        RETURN s, n\n",
    "        \"\"\"\n",
    "    n4j.query(link_sum_query, {'gid': gid})\n",
    "\n",
    "    return s\n",
    "\n",
    "def call_llm(sys, user):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys},\n",
    "            {\"role\": \"user\", \"content\": f\" {user}\"},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def find_index_of_largest(nums):\n",
    "    # Sorting the list while keeping track of the original indexes\n",
    "    sorted_with_index = sorted((num, index) for index, num in enumerate(nums))\n",
    "    \n",
    "    # Extracting the original index of the largest element\n",
    "    largest_original_index = sorted_with_index[-1][1]\n",
    "    \n",
    "    return largest_original_index\n",
    "\n",
    "def get_response(n4j, gid, query):\n",
    "    selfcont = ret_context(n4j, gid)\n",
    "    linkcont = link_context(n4j, gid)\n",
    "    user_one = \"the question is: \" + query + \"the provided information is:\" +  \"\".join(selfcont)\n",
    "    res = call_llm(sys_prompt_one,user_one)\n",
    "    user_two = \"the question is: \" + query + \"the last response of it is:\" +  res + \"the references are: \" +  \"\".join(linkcont)\n",
    "    res = call_llm(sys_prompt_two,user_two)\n",
    "    return res\n",
    "\n",
    "def link_context(n4j, gid):\n",
    "    cont = []\n",
    "    retrieve_query = \"\"\"\n",
    "        // Match all 'n' nodes with a specific gid but not of the \"Summary\" type\n",
    "        MATCH (n)\n",
    "        WHERE n.gid = $gid AND NOT n:Summary\n",
    "\n",
    "        // Find all 'm' nodes where 'm' is a reference of 'n' via a 'REFERENCES' relationship\n",
    "        MATCH (n)-[r:REFERENCE]->(m)\n",
    "        WHERE NOT m:Summary\n",
    "\n",
    "        // Find all 'o' nodes connected to each 'm', and include the relationship type,\n",
    "        // while excluding 'Summary' type nodes and 'REFERENCE' relationship\n",
    "        MATCH (m)-[s]-(o)\n",
    "        WHERE NOT o:Summary AND TYPE(s) <> 'REFERENCE'\n",
    "\n",
    "        // Collect and return details in a structured format\n",
    "        RETURN n.id AS NodeId1, \n",
    "            m.id AS Mid, \n",
    "            TYPE(r) AS ReferenceType, \n",
    "            collect(DISTINCT {RelationType: type(s), Oid: o.id}) AS Connections\n",
    "    \"\"\"\n",
    "    res = n4j.query(retrieve_query, {'gid': gid})\n",
    "    for r in res:\n",
    "        # Expand each set of connections into separate entries with n and m\n",
    "        for ind, connection in enumerate(r[\"Connections\"]):\n",
    "            cont.append(\"Reference \" + str(ind) + \": \" + r[\"NodeId1\"] + \"has the reference that\" + r['Mid'] + connection['RelationType'] + connection['Oid'])\n",
    "    return cont\n",
    "\n",
    "def ret_context(n4j, gid):\n",
    "    cont = []\n",
    "    ret_query = \"\"\"\n",
    "    // Match all nodes with a specific gid but not of type \"Summary\" and collect them\n",
    "    MATCH (n)\n",
    "    WHERE n.gid = $gid AND NOT n:Summary\n",
    "    WITH collect(n) AS nodes\n",
    "\n",
    "    // Unwind the nodes to a pairs and match relationships between them\n",
    "    UNWIND nodes AS n\n",
    "    UNWIND nodes AS m\n",
    "    MATCH (n)-[r]-(m)\n",
    "    WHERE n.gid = m.gid AND id(n) < id(m) AND NOT n:Summary AND NOT m:Summary // Ensure each pair is processed once and exclude \"Summary\" nodes in relationships\n",
    "    WITH n, m, TYPE(r) AS relType\n",
    "\n",
    "    // Return node IDs and relationship types in structured format\n",
    "    RETURN n.id AS NodeId1, relType, m.id AS NodeId2\n",
    "    \"\"\"\n",
    "    res = n4j.query(ret_query, {'gid': gid})\n",
    "    for r in res:\n",
    "        cont.append(r['NodeId1'] + r['relType'] + r['NodeId2'])\n",
    "    return cont\n",
    "\n",
    "def merge_similar_nodes(n4j, gid):\n",
    "    # Define your merge query here. Adjust labels and properties according to your graph schema\n",
    "    if gid:\n",
    "        merge_query = \"\"\"\n",
    "            WITH 0.5 AS threshold\n",
    "            MATCH (n), (m)\n",
    "            WHERE NOT n:Summary AND NOT m:Summary AND n.gid = m.gid AND n.gid = $gid AND n<>m AND apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m))\n",
    "            WITH n, m,\n",
    "                gds.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "            WHERE similarity > threshold\n",
    "            WITH head(collect([n,m])) as nodes\n",
    "            CALL apoc.refactor.mergeNodes(nodes, {properties: 'overwrite', mergeRels: true})\n",
    "            YIELD node\n",
    "            RETURN count(*)\n",
    "        \"\"\"\n",
    "        result = n4j.query(merge_query, {'gid': gid})\n",
    "    else:\n",
    "        merge_query = \"\"\"\n",
    "            // Define a threshold for cosine similarity\n",
    "            WITH 0.5 AS threshold\n",
    "            MATCH (n), (m)\n",
    "            WHERE NOT n:Summary AND NOT m:Summary AND n<>m AND apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m))\n",
    "            WITH n, m,\n",
    "                gds.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "            WHERE similarity > threshold\n",
    "            WITH head(collect([n,m])) as nodes\n",
    "            CALL apoc.refactor.mergeNodes(nodes, {properties: 'overwrite', mergeRels: true})\n",
    "            YIELD node\n",
    "            RETURN count(*)\n",
    "        \"\"\"\n",
    "        result = n4j.query(merge_query)\n",
    "    return result\n",
    "\n",
    "def ref_link(n4j, gid1, gid2):\n",
    "    trinity_query = \"\"\"\n",
    "        // Match nodes from Graph A\n",
    "        MATCH (a)\n",
    "        WHERE a.gid = $gid1 AND NOT a:Summary\n",
    "        WITH collect(a) AS GraphA\n",
    "\n",
    "        // Match nodes from Graph B\n",
    "        MATCH (b)\n",
    "        WHERE b.gid = $gid2 AND NOT b:Summary\n",
    "        WITH GraphA, collect(b) AS GraphB\n",
    "\n",
    "        // Unwind the nodes to compare each against each\n",
    "        UNWIND GraphA AS n\n",
    "        UNWIND GraphB AS m\n",
    "\n",
    "        // Set the threshold for cosine similarity\n",
    "        WITH n, m, 0.6 AS threshold\n",
    "\n",
    "        // Compute cosine similarity and apply the threshold\n",
    "        WHERE apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m)) AND n <> m\n",
    "        WITH n, m, threshold,\n",
    "            gds.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "        WHERE similarity > threshold\n",
    "\n",
    "        // Create a relationship based on the condition\n",
    "        MERGE (m)-[:REFERENCE]->(n)\n",
    "\n",
    "        // Return results\n",
    "        RETURN n, m\n",
    "\"\"\"\n",
    "    result = n4j.query(trinity_query, {'gid1': gid1, 'gid2': gid2})\n",
    "    return result\n",
    "\n",
    "\n",
    "def str_uuid():\n",
    "    # Generate a random UUID\n",
    "    generated_uuid = uuid.uuid4()\n",
    "\n",
    "    # Convert UUID to a string\n",
    "    return str(generated_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42824eb",
   "metadata": {},
   "source": [
    "# Agentic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5666f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding: 'The month is October.'\n",
      "No chunks, creating a new one\n",
      "Created new chunk (620d2): Dates & Times\n",
      "\n",
      "Adding: 'The year is 2023.'\n",
      "No chunks found\n",
      "Created new chunk (1b67e): Date & Times\n",
      "\n",
      "Adding: 'One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.'\n",
      "No chunks found\n",
      "Created new chunk (0cd9b): Performance & Success\n",
      "\n",
      "Adding: 'Teachers and coaches implicitly told us that the returns were linear.'\n",
      "No chunks found\n",
      "Created new chunk (0c700): Teaching & Coaching Assumptions\n",
      "\n",
      "Adding: 'I heard a thousand times that 'You get out what you put in.''\n",
      "No chunks found\n",
      "Created new chunk (c0d6c): Effort and Reward Principles\n",
      "\n",
      "You have 5 chunks\n",
      "\n",
      "Chunk #0\n",
      "Chunk ID: 620d2\n",
      "Summary: This chunk contains information about dates and times, specifically the month of October.\n",
      "Propositions:\n",
      "    -The month is October.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #1\n",
      "Chunk ID: 1b67e\n",
      "Summary: This chunk contains information about dates and times.\n",
      "Propositions:\n",
      "    -The year is 2023.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #2\n",
      "Chunk ID: 0cd9b\n",
      "Summary: This chunk discusses the concept of superlinear returns in relation to understanding performance and success.\n",
      "Propositions:\n",
      "    -One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #3\n",
      "Chunk ID: 0c700\n",
      "Summary: This chunk contains information about the assumption of linearity in teaching and coaching.\n",
      "Propositions:\n",
      "    -Teachers and coaches implicitly told us that the returns were linear.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #4\n",
      "Chunk ID: c0d6c\n",
      "Summary: This chunk contains common sayings or principles related to effort and reward.\n",
      "Propositions:\n",
      "    -I heard a thousand times that 'You get out what you put in.'\n",
      "\n",
      "\n",
      "\n",
      "Chunk Outline\n",
      "\n",
      "Chunk ID: 620d2\n",
      "Chunk Name: Dates & Times\n",
      "Chunk Summary: This chunk contains information about dates and times, specifically the month of October.\n",
      "\n",
      "Chunk ID: 1b67e\n",
      "Chunk Name: Date & Times\n",
      "Chunk Summary: This chunk contains information about dates and times.\n",
      "\n",
      "Chunk ID: 0cd9b\n",
      "Chunk Name: Performance & Success\n",
      "Chunk Summary: This chunk discusses the concept of superlinear returns in relation to understanding performance and success.\n",
      "\n",
      "Chunk ID: 0c700\n",
      "Chunk Name: Teaching & Coaching Assumptions\n",
      "Chunk Summary: This chunk contains information about the assumption of linearity in teaching and coaching.\n",
      "\n",
      "Chunk ID: c0d6c\n",
      "Chunk Name: Effort and Reward Principles\n",
      "Chunk Summary: This chunk contains common sayings or principles related to effort and reward.\n",
      "\n",
      "\n",
      "['The month is October.', 'The year is 2023.', \"One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\", 'Teachers and coaches implicitly told us that the returns were linear.', \"I heard a thousand times that 'You get out what you put in.'\"]\n"
     ]
    }
   ],
   "source": [
    "class AgenticChunker:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "\n",
    "        # Whether or not to update/refine summaries and titles as you get new information\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "\n",
    "        self.llm = ChatOpenAI(model=model, api_key=openai_api_key, temperature=0)\n",
    "\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "    \n",
    "    def add_proposition(self, proposition):\n",
    "        if self.print_logging:\n",
    "            print (f\"\\nAdding: '{proposition}'\")\n",
    "\n",
    "        # If it's your first chunk, just make a new chunk and don't check for others\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "\n",
    "        # If a chunk was found then add the proposition to it\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "            return\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks found\")\n",
    "            # If a chunk wasn't found, then create a new one\n",
    "            self._create_new_chunk(proposition)\n",
    "        \n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
    "        # Add then\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "\n",
    "        # Then grab a new summary\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the chunk new summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary']\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _update_chunk_title(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good title will say what the chunk is about.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
    "\n",
    "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        updated_chunk_title = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary'],\n",
    "            \"current_title\" : chunk['title']\n",
    "        }).content\n",
    "\n",
    "        return updated_chunk_title\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the new chunk summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": proposition\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _get_new_chunk_title(self, summary):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "                    You will be given a summary of a chunk which needs a title\n",
    "\n",
    "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_title = runnable.invoke({\n",
    "            \"summary\": summary\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_title\n",
    "\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id' : new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title' : new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index' : len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "    \n",
    "    def get_chunk_outline(self):\n",
    "        \"\"\"\n",
    "        Get a string which represents the chunks you currently have.\n",
    "        This will be empty when you first start off\n",
    "        \"\"\"\n",
    "        chunk_outline = \"\"\n",
    "\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ID: {chunk['chunk_id']}\\nChunk Name: {chunk['title']}\\nChunk Summary: {chunk['summary']}\\n\\n\"\"\"\n",
    "        \n",
    "            chunk_outline += single_chunk_string\n",
    "        \n",
    "        return chunk_outline\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
    "\n",
    "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
    "                    The goal is to group similar propositions and chunks.\n",
    "\n",
    "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
    "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
    "\n",
    "                    Example:\n",
    "                    Input:\n",
    "                        - Proposition: \"Greg really likes hamburgers\"\n",
    "                        - Current Chunks:\n",
    "                            - Chunk ID: 2n4l3d\n",
    "                            - Chunk Name: Places in San Francisco\n",
    "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
    "\n",
    "                            - Chunk ID: 93833k\n",
    "                            - Chunk Name: Food Greg likes\n",
    "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
    "                    Output: 93833k\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
    "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        chunk_found = runnable.invoke({\n",
    "            \"proposition\": proposition,\n",
    "            \"current_chunk_outline\": current_chunk_outline\n",
    "        }).content\n",
    "\n",
    "        # Pydantic data class\n",
    "        class ChunkID(BaseModel):\n",
    "            \"\"\"Extracting the chunk id\"\"\"\n",
    "            chunk_id: Optional[str]\n",
    "            \n",
    "        # Extraction to catch-all LLM responses. This is a bandaid\n",
    "        extraction_chain = create_extraction_chain_pydantic(pydantic_schema=ChunkID, llm=self.llm)\n",
    "        extraction_found = extraction_chain.run(chunk_found)\n",
    "        if extraction_found:\n",
    "            chunk_found = extraction_found[0].chunk_id\n",
    "\n",
    "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
    "        # So return nothing\n",
    "        if len(chunk_found) != self.id_truncate_limit:\n",
    "            return None\n",
    "\n",
    "        return chunk_found\n",
    "    \n",
    "    def get_chunks(self, get_type='dict'):\n",
    "        \"\"\"\n",
    "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
    "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
    "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
    "        \"\"\"\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            chunks = []\n",
    "            for chunk_id, chunk in self.chunks.items():\n",
    "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
    "            return chunks\n",
    "    \n",
    "    def pretty_print_chunks(self):\n",
    "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    -{prop}\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def pretty_print_chunk_outline(self):\n",
    "        print (\"Chunk Outline\\n\")\n",
    "        print(self.get_chunk_outline())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ac = AgenticChunker()\n",
    "\n",
    "    ## Comment and uncomment the propositions to your hearts content\n",
    "    propositions = [\n",
    "        'The month is October.',\n",
    "        'The year is 2023.',\n",
    "        \"One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\",\n",
    "        'Teachers and coaches implicitly told us that the returns were linear.',\n",
    "        \"I heard a thousand times that 'You get out what you put in.'\",\n",
    "    ]\n",
    "    \n",
    "    ac.add_propositions(propositions)\n",
    "    ac.pretty_print_chunks()\n",
    "    ac.pretty_print_chunk_outline()\n",
    "    print (ac.get_chunks(get_type='list_of_strings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ffb3f",
   "metadata": {},
   "source": [
    "# Clear Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ad1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def clean_graph(self):\n",
    "        with self.driver.session() as session:\n",
    "            session.execute_write(self._delete_all)\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_all(tx):\n",
    "        tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Example usage\n",
    "conn = Neo4jConnection(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "conn.clean_graph()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaaf80",
   "metadata": {},
   "source": [
    "# Data Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cfbdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "def load_high(datapath):\n",
    "    all_content = \"\"  # Initialize an empty string to hold all the content\n",
    "    with open(datapath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            all_content += line.strip() + \"\\n\"  # Append each line to the string, add newline character if needed\n",
    "    return all_content\n",
    "    \n",
    "# Pydantic data class\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "def get_propositions(text, runnable, extraction_chain):\n",
    "    runnable_output = runnable.invoke({\n",
    "    \t\"input\": text\n",
    "    }).content\n",
    "    \n",
    "    propositions = extraction_chain.run(runnable_output)[0].sentences\n",
    "    return propositions\n",
    "\n",
    "def run_chunk(essay):\n",
    "\n",
    "    obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "    llm = ChatOpenAI(model='gpt-4-1106-preview', openai_api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    runnable = obj | llm\n",
    "\n",
    "    # Extraction\n",
    "    extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n",
    "\n",
    "    paragraphs = essay.split(\"\\n\\n\")\n",
    "\n",
    "    essay_propositions = []\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        propositions = get_propositions(para, runnable, extraction_chain)\n",
    "        \n",
    "        essay_propositions.extend(propositions)\n",
    "        print (f\"Done with {i}\")\n",
    "\n",
    "    ac = AgenticChunker()\n",
    "    ac.add_propositions(essay_propositions)\n",
    "    ac.pretty_print_chunks()\n",
    "    chunks = ac.get_chunks(get_type='list_of_strings')\n",
    "\n",
    "    return chunks\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca3c3d",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa569f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_metagraph(args, content, gid, n4j):\n",
    "\n",
    "    # Set instance\n",
    "    uio = UnstructuredIO()\n",
    "    kg_agent = KnowledgeGraphAgent()\n",
    "    whole_chunk = content\n",
    "\n",
    "    if args.grained_chunk == True:\n",
    "        content = run_chunk(content)\n",
    "    else:\n",
    "        content = [content]\n",
    "    for cont in content:\n",
    "        element_example = uio.create_element_from_text(text=cont)\n",
    "\n",
    "        ans_str = kg_agent.run(element_example, parse_graph_elements=False)\n",
    "        # print(ans_str)\n",
    "\n",
    "        graph_elements = kg_agent.run(element_example, parse_graph_elements=True)\n",
    "        graph_elements = add_ge_emb(graph_elements)\n",
    "        graph_elements = add_gid(graph_elements, gid)\n",
    "\n",
    "        n4j.add_graph_elements(graph_elements=[graph_elements])\n",
    "    if args.ingraphmerge:\n",
    "        merge_similar_nodes(n4j, gid)\n",
    "    add_sum(n4j, whole_chunk, gid)\n",
    "    return n4j\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c9af5",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fd24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_p = \"\"\"\n",
    "Assess the similarity of the two provided summaries and return a rating from these options: 'very similar', 'similar', 'general', 'not similar', 'totally not similar'. Provide only the rating.\n",
    "\"\"\"\n",
    "\n",
    "def seq_ret(n4j, sumq):\n",
    "    rating_list = []\n",
    "    sumk = []\n",
    "    gids = []\n",
    "    sum_query = \"\"\"\n",
    "        MATCH (s:Summary)\n",
    "        RETURN s.content, s.gid\n",
    "        \"\"\"\n",
    "    res = n4j.query(sum_query)\n",
    "    for r in res:\n",
    "        sumk.append(r['s.content'])\n",
    "        gids.append(r['s.gid'])\n",
    "    \n",
    "    for sk in sumk:\n",
    "        sk = sk[0]\n",
    "        rate = call_llm(sys_p, \"The two summaries for comparison are: \\n Summary 1: \" + sk + \"\\n Summary 2: \" + sumq[0])\n",
    "        if \"totally not similar\" in rate:\n",
    "            rating_list.append(0)\n",
    "        elif \"not similar\" in rate:\n",
    "            rating_list.append(1)\n",
    "        elif \"general\" in rate:\n",
    "            rating_list.append(2)\n",
    "        elif \"very similar\" in rate:\n",
    "            rating_list.append(4)\n",
    "        elif \"similar\" in rate:\n",
    "            rating_list.append(3)\n",
    "        else:\n",
    "            print(\"llm returns no relevant rate\")\n",
    "            rating_list.append(-1)\n",
    "\n",
    "    ind = find_index_of_largest(rating_list)\n",
    "    # print('ind is', ind)\n",
    "\n",
    "    gid = gids[ind]\n",
    "\n",
    "    return gid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c19fe",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59d19da",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './prompt.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Set Neo4j instance\u001b[39;00m\n\u001b[32m      2\u001b[39m n4j = Neo4jGraph(\n\u001b[32m      3\u001b[39m     url=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mNEO4J_URL\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m     username=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mNEO4J_USERNAME\u001b[39m\u001b[33m\"\u001b[39m),             \u001b[38;5;66;03m# Default username\u001b[39;00m\n\u001b[32m      5\u001b[39m     password=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mNEO4J_PASSWORD\u001b[39m\u001b[33m\"\u001b[39m)     \u001b[38;5;66;03m# Replace 'yourpassword' with your actual password\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m question = \u001b[43mload_high\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./prompt.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28msum\u001b[39m = process_chunks(question)\n\u001b[32m     10\u001b[39m gid = seq_ret(n4j, \u001b[38;5;28msum\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_high\u001b[39m\u001b[34m(datapath)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_high\u001b[39m(datapath):\n\u001b[32m      3\u001b[39m     all_content = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Initialize an empty string to hold all the content\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[32m      6\u001b[39m             all_content += line.strip() + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Append each line to the string, add newline character if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/NgJaBach/Medical-Graph-RAG/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './prompt.txt'"
     ]
    }
   ],
   "source": [
    "# Set Neo4j instance\n",
    "n4j = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),             # Default username\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")     # Replace 'yourpassword' with your actual password\n",
    ")\n",
    "\n",
    "question = load_high(\"./prompt.txt\")\n",
    "sum = process_chunks(question)\n",
    "gid = seq_ret(n4j, sum)\n",
    "response = get_response(n4j, gid, question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
