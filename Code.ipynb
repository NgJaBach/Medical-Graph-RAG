{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a57e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ngjabach/Documents/NgJaBach/Medical-Graph-RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from camel.storages import Neo4jGraph\n",
    "from camel.agents import KnowledgeGraphAgent\n",
    "from camel.loaders import UnstructuredIO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import repeat\n",
    "import shortuuid\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import backoff\n",
    "import openai\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")\n",
    "\n",
    "n4j = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = \"gpt-4.1-nano\"\n",
    "embedder_name = \"text-embedding-3-small\"\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=999, max_tries=99)\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return client.chat.completions.create(**kwargs)\n",
    "\n",
    "def ask_gpt(user, sys) -> str:\n",
    "    response = completions_with_backoff(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        max_tokens=5000,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedder_name\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "threshold = 0.8\n",
    "def cosine_similarity_paragraphs(paragraph1: str, paragraph2: str) -> float:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([paragraph1, paragraph2])\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return float(sim_matrix[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42824eb",
   "metadata": {},
   "source": [
    "# Agentic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(file, message):\n",
    "    with open(file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "\n",
    "def clearing(file):\n",
    "    with open(file, \"w\") as f:\n",
    "        f.write(\"\")\n",
    "\n",
    "class AgenticChunker:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}\n",
    "        self.llm = ChatOpenAI(model=model_name, api_key=openai_api_key, temperature=0)\n",
    "\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in tqdm(propositions, desc=\"Adding propositions to chunks\", unit=\"proposition\"):\n",
    "            if len(self.chunks) == 0:\n",
    "                self._create_new_chunk(proposition)\n",
    "                continue\n",
    "            chunk_id = self._find_relevant_chunk(proposition)\n",
    "            if chunk_id:\n",
    "                self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "            else:\n",
    "                self._create_new_chunk(proposition)\n",
    "        \n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the new chunk summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": proposition\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _get_new_chunk_title(self, summary):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "                    You will be given a summary of a chunk which needs a title\n",
    "\n",
    "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_title = runnable.invoke({\n",
    "            \"summary\": summary\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_title\n",
    "\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(shortuuid.uuid())\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id' : new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title' : new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index' : len(self.chunks)\n",
    "        }\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        chunk_found = None\n",
    "        similariest = 0\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            id = chunk['chunk_id'] \n",
    "            content = chunk['title'] + chunk['summary']\n",
    "            sim = cosine_similarity_paragraphs(content, proposition)\n",
    "            if sim > similariest:\n",
    "                similariest = sim\n",
    "                chunk_found = id\n",
    "        if similariest < threshold:\n",
    "            return None\n",
    "        return chunk_found\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        chunks = []\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
    "        return chunks\n",
    "    \n",
    "    def pretty_print_chunks(self):\n",
    "        clearing(\"current_chunks.txt\")\n",
    "        logging(\"current_chunks.txt\", f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            logging(\"current_chunks.txt\", f\"Chunk #{chunk['chunk_index']}\")\n",
    "            logging(\"current_chunks.txt\", f\"Chunk ID: {chunk_id}\")\n",
    "            logging(\"current_chunks.txt\", f\"Summary: {chunk['summary']}\")\n",
    "            logging(\"current_chunks.txt\", f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                logging(\"current_chunks.txt\", f\"    -{prop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa71ec",
   "metadata": {},
   "source": [
    "# Summerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd561aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prompt = \"\"\"\n",
    "Generate a structured summary from the provided medical source (report, paper, or book), strictly adhering to the following categories. The summary should list key information under each category in a concise format: 'CATEGORY_NAME: Key information'. No additional explanations or detailed descriptions are necessary unless directly related to the categories:\n",
    "Each category should be addressed only if relevant to the content of the medical source. Ensure the summary is clear and direct, suitable for quick reference.\n",
    "\n",
    "ANATOMICAL_STRUCTURE: Mention any anatomical structures specifically discussed.\n",
    "BODY_FUNCTION: List any body functions highlighted.\n",
    "BODY_MEASUREMENT: Include normal measurements like blood pressure or temperature.\n",
    "BM_RESULT: Results of these measurements.\n",
    "BM_UNIT: Units for each measurement.\n",
    "BM_VALUE: Values of these measurements.\n",
    "LABORATORY_DATA: Outline any laboratory tests mentioned.\n",
    "LAB_RESULT: Outcomes of these tests (e.g., 'increased', 'decreased').\n",
    "LAB_VALUE: Specific values from the tests.\n",
    "LAB_UNIT: Units of measurement for these values.\n",
    "MEDICINE: Name medications discussed.\n",
    "MED_DOSE, MED_DURATION, MED_FORM, MED_FREQUENCY, MED_ROUTE, MED_STATUS, MED_STRENGTH, MED_UNIT, MED_TOTALDOSE: Provide concise details for each medication attribute.\n",
    "PROBLEM: Identify any medical conditions or findings.\n",
    "PROCEDURE: Describe any procedures.\n",
    "PROCEDURE_RESULT: Outcomes of these procedures.\n",
    "PROC_METHOD: Methods used.\n",
    "SEVERITY: Severity of the conditions mentioned.\n",
    "MEDICAL_DEVICE: List any medical devices used.\n",
    "SUBSTANCE_ABUSE: Note any substance abuse mentioned.\n",
    "\"\"\"\n",
    "\n",
    "def chunking(docs, chunk_size=32768, chunk_overlap=1024):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_text(docs)\n",
    "\n",
    "def process_chunks(content: str):\n",
    "    chunks = chunking(content)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        responses = list(executor.map(ask_gpt, chunks, repeat(sum_prompt)))\n",
    "    return responses\n",
    "\n",
    "def run_chunk(docs, begin=0):\n",
    "    propositions = chunking(docs)\n",
    "    ac = AgenticChunker()\n",
    "    ac.add_propositions(propositions)\n",
    "    ac.pretty_print_chunks()\n",
    "    chunks = ac.get_chunks()\n",
    "    return chunks[begin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca3c3d",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa569f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ge_emb(graph_element):\n",
    "    for node in graph_element.nodes:\n",
    "        emb = get_embedding(node.id)\n",
    "        node.properties['embedding'] = emb\n",
    "    return graph_element\n",
    "\n",
    "def add_gid(graph_element, gid):\n",
    "    for node in graph_element.nodes:\n",
    "        node.properties['gid'] = gid\n",
    "    for rel in graph_element.relationships:\n",
    "        rel.properties['gid'] = gid\n",
    "    return graph_element\n",
    "\n",
    "def link_sum(gid):\n",
    "    link_sum_query = \"\"\"\n",
    "        CALL apoc.periodic.iterate(\n",
    "        '\n",
    "            MATCH (s:Summary {gid: $gid}), (n)\n",
    "            WHERE n.gid = $gid AND NOT n:Summary\n",
    "            RETURN s, n\n",
    "        ',\n",
    "        '\n",
    "            MERGE (s)-[:SUMMARIZES]->(n)\n",
    "        ',\n",
    "        {\n",
    "            batchSize: 5000,\n",
    "            parallel: false,\n",
    "            params: { gid: $gid }\n",
    "        }\n",
    "        );\n",
    "        \"\"\"\n",
    "    n4j.query(link_sum_query, {'gid': gid})\n",
    "\n",
    "def reset_sum():\n",
    "    rmv_dups_query = \"\"\"\n",
    "        MATCH (n:Summary)-[r:SUMMARIZES]->()\n",
    "        DELETE r\n",
    "        \"\"\"\n",
    "    S = n4j.query(rmv_dups_query)\n",
    "    print(\"Done removing duplicates!\")\n",
    "    list_gids = n4j.query(\"MATCH (n) RETURN DISTINCT n.gid\")\n",
    "    list_gids = [gid['n.gid'] for gid in list_gids]\n",
    "    for gid in tqdm(list_gids, desc=\"Re-adding summaries\", unit=\"gid\"):\n",
    "        link_sum(gid)\n",
    "    print(\"Done re-adding summaries!\")\n",
    "\n",
    "def add_sum(n4j,content,gid):\n",
    "    sum = process_chunks(content)\n",
    "    # print(\"Summary: \", sum)\n",
    "    creat_sum_query = \"\"\"\n",
    "        CREATE (s:Summary {content: $sum, gid: $gid})\n",
    "        RETURN s\n",
    "        \"\"\"\n",
    "    n4j.query(creat_sum_query, {'sum': sum, 'gid': gid})\n",
    "    link_sum(gid)\n",
    "\n",
    "def creat_metagraph(content, gid, n4j, begin=0):\n",
    "    print(f\"gid: {gid}\")\n",
    "    uio = UnstructuredIO()\n",
    "    kg_agent = KnowledgeGraphAgent()\n",
    "    whole_chunk = content\n",
    "    saved_chunks = \"/home/ngjabach/Documents/NgJaBach/Medical-Graph-RAG/half_baked/\" + str(gid) + \".txt\"\n",
    "    if os.path.exists(saved_chunks):\n",
    "        content = []\n",
    "        with open(saved_chunks, 'r') as file:\n",
    "            for line in file:\n",
    "                content.append(line.strip())\n",
    "    else:\n",
    "        content = run_chunk(content, begin)\n",
    "        with open(saved_chunks, 'w') as file:\n",
    "            for item in content:\n",
    "                file.write(str(item) + '\\n')\n",
    "    for cont in tqdm(content, desc=\"Processing chunks\", unit=\"chunk\"):\n",
    "        element_example = uio.create_element_from_text(text=cont)\n",
    "        graph_elements = kg_agent.run(element_example, parse_graph_elements=True)\n",
    "        graph_elements = add_ge_emb(graph_elements)\n",
    "        graph_elements = add_gid(graph_elements, gid)\n",
    "        n4j.add_graph_elements(graph_elements=[graph_elements])\n",
    "\n",
    "    add_sum(n4j, whole_chunk, gid)\n",
    "    print(\"God blessed us all!\")\n",
    "    return n4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42d646",
   "metadata": {},
   "source": [
    "# Data Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94079ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_link(n4j, gid1, gid2):\n",
    "    trinity_query = \"\"\"\n",
    "        MATCH (a)\n",
    "        WHERE a.gid = $gid1 AND NOT a:Summary\n",
    "        WITH collect(a) AS GraphA\n",
    "\n",
    "        MATCH (b)\n",
    "        WHERE b.gid = $gid2 AND NOT b:Summary\n",
    "        WITH GraphA, collect(b) AS GraphB\n",
    "\n",
    "        UNWIND GraphA AS n\n",
    "        UNWIND GraphB AS m\n",
    "\n",
    "        WITH n, m, $threshold AS threshold\n",
    "        WHERE apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m)) AND n <> m\n",
    "        WITH n, m, threshold,\n",
    "            vector.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "        WHERE similarity > threshold\n",
    "        MERGE (m)-[:REFERENCE]->(n)\n",
    "\n",
    "        RETURN n, m\n",
    "\"\"\"\n",
    "    result = n4j.query(trinity_query, {'gid1': gid1, 'gid2': gid2, 'threshold': threshold})\n",
    "    return result\n",
    "\n",
    "def check(file_name):\n",
    "    with open(\"Done.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "        taboo = content.split()\n",
    "    if file_name in taboo:\n",
    "        print(f\"Skipping {file_name} as it is already processed.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def mega_load(n4j, data_path, k=-1, msg=\"files\"):\n",
    "    files = [file for file in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, file))]\n",
    "    if k == -1:\n",
    "        k = len(files)\n",
    "    print(f\"Number of {msg}: {len(files)}\")\n",
    "    print(f\"Randomly selecting {k} {msg}...\")\n",
    "    k_selected = random.sample(files, k)\n",
    "    k_selected = sorted(k_selected)\n",
    "    print(f\"Selected {k} {msg}: {k_selected}\")\n",
    "    for i, file_name in enumerate(k_selected):\n",
    "        if not check(file_name):\n",
    "            continue\n",
    "        print(f\"Processing {i + 1}th {msg}: {file_name}\")\n",
    "        content = \"\"\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                content += line.strip() + \"\\n\"\n",
    "        gid = str(shortuuid.uuid())\n",
    "        n4j = creat_metagraph(content, gid, n4j)\n",
    "        logging(\"Done.txt\", file_name)\n",
    "    list_gids = n4j.query(\"MATCH (n) RETURN DISTINCT n.gid\")\n",
    "    list_gids = [gid['n.gid'] for gid in list_gids]\n",
    "    for i in range(len(list_gids)):\n",
    "        for j in range(0, i):\n",
    "            gid1 = list_gids[i]\n",
    "            gid2 = list_gids[j]\n",
    "            ref_link(n4j, gid1, gid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c19fe",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Processing first floor...\")\n",
    "# mega_load(n4j, \"./patients\", k=5, msg=\"patient(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782fe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def half_baked(file_path, gid, n4j):\n",
    "#     list_gids = n4j.query(\"MATCH (n) RETURN DISTINCT n.gid\")\n",
    "#     list_gids = [gid['n.gid'] for gid in list_gids]\n",
    "#     content = \"\"\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         for line in file:\n",
    "#             content += line.strip() + \"\\n\"\n",
    "#     n4j = creat_metagraph(content, gid, n4j, 509+455)\n",
    "\n",
    "# half_baked(\"./books_MEDQA/Pathology_Robbins.txt\", \"e9VJYZmhXzdrr2PXAqmrne\", n4j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b083b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Processing second floor...\")\n",
    "# mega_load(n4j, \"./books_MEDQA/\", msg=\"book(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6259399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_gids = n4j.query(\"MATCH (n) RETURN DISTINCT n.gid\")\n",
    "# list_gids = [gid['n.gid'] for gid in list_gids]\n",
    "# for i in range(len(list_gids)):\n",
    "#     for j in range(0, i):\n",
    "#         print(f\"Linking {i} and {j}\")\n",
    "#         gid1 = list_gids[i]\n",
    "#         gid2 = list_gids[j]\n",
    "#         ref_link(n4j, gid1, gid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4392d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_context(n4j, gid):\n",
    "    cont = []\n",
    "    retrieve_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n.gid = $gid AND NOT n:Summary\n",
    "\n",
    "        MATCH (n)-[r:REFERENCE]->(m)\n",
    "        WHERE NOT m:Summary\n",
    "\n",
    "        MATCH (m)-[s]-(o)\n",
    "        WHERE NOT o:Summary AND TYPE(s) <> 'REFERENCE'\n",
    "\n",
    "        RETURN n.id AS NodeId1, \n",
    "            m.id AS Mid, \n",
    "            TYPE(r) AS ReferenceType, \n",
    "            collect(DISTINCT {RelationType: type(s), Oid: o.id}) AS Connections\n",
    "    \"\"\"\n",
    "    res = n4j.query(retrieve_query, {'gid': gid})\n",
    "    for r in res:\n",
    "        for ind, connection in enumerate(r[\"Connections\"]):\n",
    "            cont.append(\"Reference \" + str(ind) + \": \" + r[\"NodeId1\"] + \"has the reference that\" + r['Mid'] + connection['RelationType'] + connection['Oid'])\n",
    "    return cont\n",
    "\n",
    "def ret_context(n4j, gid):\n",
    "    cont = []\n",
    "    ret_query = \"\"\"\n",
    "    // Match all nodes with a specific gid but not of type \"Summary\" and collect them\n",
    "    MATCH (n)\n",
    "    WHERE n.gid = $gid AND NOT n:Summary\n",
    "    WITH collect(n) AS nodes\n",
    "\n",
    "    // Unwind the nodes to a pairs and match relationships between them\n",
    "    UNWIND nodes AS n\n",
    "    UNWIND nodes AS m\n",
    "    MATCH (n)-[r]-(m)\n",
    "    WHERE n.gid = m.gid AND elementId(n) < elementId(m) AND NOT n:Summary AND NOT m:Summary // Ensure each pair is processed once and exclude \"Summary\" nodes in relationships\n",
    "    WITH n, m, TYPE(r) AS relType\n",
    "\n",
    "    // Return node IDs and relationship types in structured format\n",
    "    RETURN n.id AS NodeId1, relType, m.id AS NodeId2\n",
    "    \"\"\"\n",
    "    res = n4j.query(ret_query, {'gid': gid})\n",
    "    for r in res:\n",
    "        cont.append(r['NodeId1'] + r['relType'] + r['NodeId2'])\n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421e61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 52/1273 [27:00<10:34:06, 31.16s/it] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_index_of_largest(nums):\n",
    "    sorted_with_index = sorted((num, index) for index, num in enumerate(nums)) # Sorting the list while keeping track of the original indexes\n",
    "    largest_original_index = sorted_with_index[-1][1] # Extracting the original index of the largest element\n",
    "    return largest_original_index\n",
    "sys_p = \"\"\"\n",
    "Assess the similarity of the two provided summaries and return a rating from these options: 'very similar', 'similar', 'general', 'not similar', 'totally not similar'. Provide only the rating.\n",
    "\"\"\"\n",
    "def seq_ret(n4j, sumq):\n",
    "    rating_list = []\n",
    "    sumk = []\n",
    "    gids = []\n",
    "    sum_query = \"\"\"\n",
    "        MATCH (s:Summary)\n",
    "        RETURN s.content, s.gid\n",
    "        \"\"\"\n",
    "    res = n4j.query(sum_query)\n",
    "    for r in res:\n",
    "        sumk.append(r['s.content'])\n",
    "        gids.append(r['s.gid'])\n",
    "    \n",
    "    for sk in sumk:\n",
    "        sk = sk[0]\n",
    "        rate = ask_gpt(\"The two summaries for comparison are: \\n Summary 1: \" + sk + \"\\n Summary 2: \" + sumq[0], sys_p)\n",
    "        if \"totally not similar\" in rate:\n",
    "            rating_list.append(0)\n",
    "        elif \"not similar\" in rate:\n",
    "            rating_list.append(1)\n",
    "        elif \"general\" in rate:\n",
    "            rating_list.append(2)\n",
    "        elif \"very similar\" in rate:\n",
    "            rating_list.append(4)\n",
    "        elif \"similar\" in rate:\n",
    "            rating_list.append(3)\n",
    "        else:\n",
    "            print(\"llm returns no relevant rate\")\n",
    "            rating_list.append(-1)\n",
    "\n",
    "    ind = find_index_of_largest(rating_list)\n",
    "    gid = gids[ind]\n",
    "    \n",
    "    return gid\n",
    "\n",
    "sys = \"\"\"\n",
    "Please answer the question using your knowledge and leveraging the additional information and references.\n",
    "Return a single letter \"A\", \"B\", \"C\" or \"D\" corresponding to the correct answer.\n",
    "Do not return any other text.\n",
    "\"\"\"\n",
    "\n",
    "def answer_llm(prompt: str) -> str:\n",
    "    response = completions_with_backoff(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": sys\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1,\n",
    "        n=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_response(n4j, gid, query):\n",
    "    selfcont = ret_context(n4j, gid)\n",
    "    linkcont = link_context(n4j, gid)\n",
    "    user_zero = f\"\"\"\n",
    "    The question is: {query}\n",
    "    The provided information is: {selfcont}\n",
    "    The references are: {linkcont}\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        res = answer_llm(user_zero)\n",
    "        if res in ['A', 'B', 'C', 'D']:\n",
    "            break\n",
    "    return res\n",
    "\n",
    "df = pd.read_json(\"MedQA.jsonl\", lines=True)\n",
    "\n",
    "# correct = 0\n",
    "# for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     q = row['question']\n",
    "#     a = row['options']\n",
    "#     s = row['answer_idx']\n",
    "#     # QA = f\"Q: {q}\\nA: {a['A']}\\nB: {a['B']}\\nC: {a['C']}\\nD: {a['D']}\\nAnswer: {s}\"\n",
    "#     # print(QA)\n",
    "#     brompt = f\"Question: {q}\\nA: {a['A']}\\nB: {a['B']}\\nC: {a['C']}\\nD: {a['D']}\\nAnswer:\"\n",
    "#     while True:\n",
    "#         sum = process_chunks(brompt)\n",
    "#         gid = seq_ret(n4j, sum)\n",
    "#         res = get_response(n4j, gid, brompt)\n",
    "#         if res in ['A', 'B', 'C', 'D']:\n",
    "#             break\n",
    "#     if res == s:\n",
    "#         correct += 1\n",
    "\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_row(row):\n",
    "    q, a, s = row['question'], row['options'], row['answer_idx']\n",
    "    brompt = (f\"Question: {q}\\n\"\n",
    "              f\"A: {a['A']}\\nB: {a['B']}\\n\"\n",
    "              f\"C: {a['C']}\\nD: {a['D']}\\nAnswer:\")\n",
    "    sum_ = process_chunks(brompt)\n",
    "    gid = seq_ret(n4j, sum_)\n",
    "    res = get_response(n4j, gid, brompt)\n",
    "    return res == s\n",
    "\n",
    "def init_worker():\n",
    "    # Reconnect or reinitialize your Neo4j session here if needed\n",
    "    global n4j\n",
    "    n4j = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "def run_parallel_processes(df, max_workers=4):\n",
    "    rows = [row for _, row in df.iterrows()]\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers,\n",
    "                                                initializer=init_worker) as executor:\n",
    "        results = list(tqdm(executor.map(process_row, rows),\n",
    "                            total=len(rows), desc=\"Processing\"))\n",
    "    return sum(results)\n",
    "\n",
    "# Usage\n",
    "correct = run_parallel_processes(df)\n",
    "\n",
    "accuracy = correct / len(df) * 100\n",
    "print(f\"Total: {len(df)}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
