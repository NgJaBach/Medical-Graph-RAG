{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a57e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from pydantic import BaseModel\n",
    "from langchain import hub\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from camel.storages import Neo4jGraph\n",
    "from camel.agents import KnowledgeGraphAgent\n",
    "from camel.loaders import UnstructuredIO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tiktoken\n",
    "import uuid\n",
    "from typing import Optional, List\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "LANGSMITH_TRACING=True\n",
    "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGSMITH_API_KEY=\"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\"\n",
    "LANGSMITH_PROJECT=\"pr-medicalgraphrag\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6db1cc",
   "metadata": {},
   "source": [
    "# Summerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299790f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "sum_prompt = \"\"\"\n",
    "Generate a structured summary from the provided medical source (report, paper, or book), strictly adhering to the following categories. The summary should list key information under each category in a concise format: 'CATEGORY_NAME: Key information'. No additional explanations or detailed descriptions are necessary unless directly related to the categories:\n",
    "\n",
    "ANATOMICAL_STRUCTURE: Mention any anatomical structures specifically discussed.\n",
    "BODY_FUNCTION: List any body functions highlighted.\n",
    "BODY_MEASUREMENT: Include normal measurements like blood pressure or temperature.\n",
    "BM_RESULT: Results of these measurements.\n",
    "BM_UNIT: Units for each measurement.\n",
    "BM_VALUE: Values of these measurements.\n",
    "LABORATORY_DATA: Outline any laboratory tests mentioned.\n",
    "LAB_RESULT: Outcomes of these tests (e.g., 'increased', 'decreased').\n",
    "LAB_VALUE: Specific values from the tests.\n",
    "LAB_UNIT: Units of measurement for these values.\n",
    "MEDICINE: Name medications discussed.\n",
    "MED_DOSE, MED_DURATION, MED_FORM, MED_FREQUENCY, MED_ROUTE, MED_STATUS, MED_STRENGTH, MED_UNIT, MED_TOTALDOSE: Provide concise details for each medication attribute.\n",
    "PROBLEM: Identify any medical conditions or findings.\n",
    "PROCEDURE: Describe any procedures.\n",
    "PROCEDURE_RESULT: Outcomes of these procedures.\n",
    "PROC_METHOD: Methods used.\n",
    "SEVERITY: Severity of the conditions mentioned.\n",
    "MEDICAL_DEVICE: List any medical devices used.\n",
    "SUBSTANCE_ABUSE: Note any substance abuse mentioned.\n",
    "Each category should be addressed only if relevant to the content of the medical source. Ensure the summary is clear and direct, suitable for quick reference.\n",
    "\"\"\"\n",
    "\n",
    "# Add your own OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = \"gpt-4.1-nano\"\n",
    "embedding = \"text-embedding-3-small\"\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_gpt(chunk) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sum_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\" {chunk}\"},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def split_into_chunks(text, tokens=500):\n",
    "    encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "    words = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), tokens):\n",
    "        chunks.append(' '.join(encoding.decode(words[i:i + tokens])))\n",
    "    return chunks   \n",
    "\n",
    "def process_chunks(content):\n",
    "    chunks = split_into_chunks(content)\n",
    "\n",
    "    # Processes chunks in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        responses = list(executor.map(ask_gpt, chunks))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0f616",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt_one = \"\"\"\n",
    "Please answer the question using insights supported by provided graph-based data relevant to medical information.\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_two = \"\"\"\n",
    "Modify the response to the question using the provided references. Include precise citations relevant to your answer. You may use multiple citations simultaneously, denoting each with the reference index number. For example, cite the first and third documents as [1][3]. If the references do not pertain to the response, simply provide a concise answer to the original question.\n",
    "\"\"\"\n",
    "\n",
    "def fetch_texts(n4j):\n",
    "    # Fetch the text for each node\n",
    "    query = \"MATCH (n) RETURN n.id AS id\"\n",
    "    return n4j.query(query)\n",
    "\n",
    "def add_embeddings(n4j, node_id, embedding):\n",
    "    # Upload embeddings to Neo4j\n",
    "    query = \"MATCH (n) WHERE n.id = $node_id SET n.embedding = $embedding\"\n",
    "    n4j.query(query, params = {\"node_id\":node_id, \"embedding\":embedding})\n",
    "\n",
    "def add_nodes_emb(n4j):\n",
    "    nodes = fetch_texts(n4j)\n",
    "    for node in nodes:\n",
    "        # Calculate embedding for each node's text\n",
    "        if node['id']:  # Ensure there is text to process\n",
    "            embedding = get_embedding(node['id'])\n",
    "            # Store embedding back in the node\n",
    "            add_embeddings(n4j, node['id'], embedding)\n",
    "\n",
    "def add_ge_emb(graph_element):\n",
    "    for node in graph_element.nodes:\n",
    "        emb = get_embedding(node.id)\n",
    "        node.properties['embedding'] = emb\n",
    "    return graph_element\n",
    "\n",
    "def add_gid(graph_element, gid):\n",
    "    for node in graph_element.nodes:\n",
    "        node.properties['gid'] = gid\n",
    "    for rel in graph_element.relationships:\n",
    "        rel.properties['gid'] = gid\n",
    "    return graph_element\n",
    "\n",
    "def add_sum(n4j,content,gid):\n",
    "    sum = process_chunks(content)\n",
    "    creat_sum_query = \"\"\"\n",
    "        CREATE (s:Summary {content: $sum, gid: $gid})\n",
    "        RETURN s\n",
    "        \"\"\"\n",
    "    s = n4j.query(creat_sum_query, {'sum': sum, 'gid': gid})\n",
    "    \n",
    "    link_sum_query = \"\"\"\n",
    "        MATCH (s:Summary {gid: $gid}), (n)\n",
    "        WHERE n.gid = s.gid AND NOT n:Summary\n",
    "        CREATE (s)-[:SUMMARIZES]->(n)\n",
    "        RETURN s, n\n",
    "        \"\"\"\n",
    "    n4j.query(link_sum_query, {'gid': gid})\n",
    "\n",
    "    return s\n",
    "\n",
    "def find_index_of_largest(nums):\n",
    "    # Sorting the list while keeping track of the original indexes\n",
    "    sorted_with_index = sorted((num, index) for index, num in enumerate(nums))\n",
    "    \n",
    "    # Extracting the original index of the largest element\n",
    "    largest_original_index = sorted_with_index[-1][1]\n",
    "    \n",
    "    return largest_original_index\n",
    "\n",
    "def get_response(n4j, gid, query):\n",
    "    selfcont = ret_context(n4j, gid)\n",
    "    linkcont = link_context(n4j, gid)\n",
    "    user_one = \"the question is: \" + query + \"the provided information is:\" +  \"\".join(selfcont)\n",
    "    res = ask_gpt(sys_prompt_one,user_one)\n",
    "    user_two = \"the question is: \" + query + \"the last response of it is:\" +  res + \"the references are: \" +  \"\".join(linkcont)\n",
    "    res = ask_gpt(sys_prompt_two,user_two)\n",
    "    return res\n",
    "\n",
    "def link_context(n4j, gid):\n",
    "    cont = []\n",
    "    retrieve_query = \"\"\"\n",
    "        // Match all 'n' nodes with a specific gid but not of the \"Summary\" type\n",
    "        MATCH (n)\n",
    "        WHERE n.gid = $gid AND NOT n:Summary\n",
    "\n",
    "        // Find all 'm' nodes where 'm' is a reference of 'n' via a 'REFERENCES' relationship\n",
    "        MATCH (n)-[r:REFERENCE]->(m)\n",
    "        WHERE NOT m:Summary\n",
    "\n",
    "        // Find all 'o' nodes connected to each 'm', and include the relationship type,\n",
    "        // while excluding 'Summary' type nodes and 'REFERENCE' relationship\n",
    "        MATCH (m)-[s]-(o)\n",
    "        WHERE NOT o:Summary AND TYPE(s) <> 'REFERENCE'\n",
    "\n",
    "        // Collect and return details in a structured format\n",
    "        RETURN n.id AS NodeId1, \n",
    "            m.id AS Mid, \n",
    "            TYPE(r) AS ReferenceType, \n",
    "            collect(DISTINCT {RelationType: type(s), Oid: o.id}) AS Connections\n",
    "    \"\"\"\n",
    "    res = n4j.query(retrieve_query, {'gid': gid})\n",
    "    for r in res:\n",
    "        # Expand each set of connections into separate entries with n and m\n",
    "        for ind, connection in enumerate(r[\"Connections\"]):\n",
    "            cont.append(\"Reference \" + str(ind) + \": \" + r[\"NodeId1\"] + \"has the reference that\" + r['Mid'] + connection['RelationType'] + connection['Oid'])\n",
    "    return cont\n",
    "\n",
    "def ret_context(n4j, gid):\n",
    "    cont = []\n",
    "    ret_query = \"\"\"\n",
    "    // Match all nodes with a specific gid but not of type \"Summary\" and collect them\n",
    "    MATCH (n)\n",
    "    WHERE n.gid = $gid AND NOT n:Summary\n",
    "    WITH collect(n) AS nodes\n",
    "\n",
    "    // Unwind the nodes to a pairs and match relationships between them\n",
    "    UNWIND nodes AS n\n",
    "    UNWIND nodes AS m\n",
    "    MATCH (n)-[r]-(m)\n",
    "    WHERE n.gid = m.gid AND id(n) < id(m) AND NOT n:Summary AND NOT m:Summary // Ensure each pair is processed once and exclude \"Summary\" nodes in relationships\n",
    "    WITH n, m, TYPE(r) AS relType\n",
    "\n",
    "    // Return node IDs and relationship types in structured format\n",
    "    RETURN n.id AS NodeId1, relType, m.id AS NodeId2\n",
    "    \"\"\"\n",
    "    res = n4j.query(ret_query, {'gid': gid})\n",
    "    for r in res:\n",
    "        cont.append(r['NodeId1'] + r['relType'] + r['NodeId2'])\n",
    "    return cont\n",
    "\n",
    "def merge_similar_nodes(n4j, gid):\n",
    "    # Define your merge query here. Adjust labels and properties according to your graph schema\n",
    "    if gid:\n",
    "        merge_query = \"\"\"\n",
    "            WITH 0.5 AS threshold\n",
    "            MATCH (n), (m)\n",
    "            WHERE NOT n:Summary AND NOT m:Summary AND n.gid = m.gid AND n.gid = $gid AND n<>m AND apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m))\n",
    "            WITH n, m,\n",
    "                vector.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "            WHERE similarity > threshold\n",
    "            WITH head(collect([n,m])) as nodes\n",
    "            CALL apoc.refactor.mergeNodes(nodes, {properties: 'overwrite', mergeRels: true})\n",
    "            YIELD node\n",
    "            RETURN count(*)\n",
    "        \"\"\"\n",
    "        result = n4j.query(merge_query, {'gid': gid})\n",
    "    else:\n",
    "        merge_query = \"\"\"\n",
    "            // Define a threshold for cosine similarity\n",
    "            WITH 0.5 AS threshold\n",
    "            MATCH (n), (m)\n",
    "            WHERE NOT n:Summary AND NOT m:Summary AND n<>m AND apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m))\n",
    "            WITH n, m,\n",
    "                vector.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "            WHERE similarity > threshold\n",
    "            WITH head(collect([n,m])) as nodes\n",
    "            CALL apoc.refactor.mergeNodes(nodes, {properties: 'overwrite', mergeRels: true})\n",
    "            YIELD node\n",
    "            RETURN count(*)\n",
    "        \"\"\"\n",
    "        result = n4j.query(merge_query)\n",
    "    return result\n",
    "\n",
    "def ref_link(n4j, gid1, gid2):\n",
    "    trinity_query = \"\"\"\n",
    "        // Match nodes from Graph A\n",
    "        MATCH (a)\n",
    "        WHERE a.gid = $gid1 AND NOT a:Summary\n",
    "        WITH collect(a) AS GraphA\n",
    "\n",
    "        // Match nodes from Graph B\n",
    "        MATCH (b)\n",
    "        WHERE b.gid = $gid2 AND NOT b:Summary\n",
    "        WITH GraphA, collect(b) AS GraphB\n",
    "\n",
    "        // Unwind the nodes to compare each against each\n",
    "        UNWIND GraphA AS n\n",
    "        UNWIND GraphB AS m\n",
    "\n",
    "        // Set the threshold for cosine similarity\n",
    "        WITH n, m, 0.6 AS threshold\n",
    "\n",
    "        // Compute cosine similarity and apply the threshold\n",
    "        WHERE apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m)) AND n <> m\n",
    "        WITH n, m, threshold,\n",
    "            vector.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "        WHERE similarity > threshold\n",
    "\n",
    "        // Create a relationship based on the condition\n",
    "        MERGE (m)-[:REFERENCE]->(n)\n",
    "\n",
    "        // Return results\n",
    "        RETURN n, m\n",
    "\"\"\"\n",
    "    result = n4j.query(trinity_query, {'gid1': gid1, 'gid2': gid2})\n",
    "    return result\n",
    "\n",
    "\n",
    "def str_uuid():\n",
    "    # Generate a random UUID\n",
    "    generated_uuid = uuid.uuid4()\n",
    "\n",
    "    # Convert UUID to a string\n",
    "    return str(generated_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42824eb",
   "metadata": {},
   "source": [
    "# Agentic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticChunker:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "\n",
    "        # Whether or not to update/refine summaries and titles as you get new information\n",
    "        self.generate_new_metadata_ind = True\n",
    "        self.print_logging = True\n",
    "\n",
    "        self.llm = ChatOpenAI(model=model, api_key=openai_api_key, temperature=0)\n",
    "\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "    \n",
    "    def add_proposition(self, proposition):\n",
    "        if self.print_logging:\n",
    "            print (f\"\\nAdding: '{proposition}'\")\n",
    "\n",
    "        # If it's your first chunk, just make a new chunk and don't check for others\n",
    "        if len(self.chunks) == 0:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks, creating a new one\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "\n",
    "        # If a chunk was found then add the proposition to it\n",
    "        if chunk_id:\n",
    "            if self.print_logging:\n",
    "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "            return\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print (\"No chunks found\")\n",
    "            # If a chunk wasn't found, then create a new one\n",
    "            self._create_new_chunk(proposition)\n",
    "        \n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
    "        # Add then\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "\n",
    "        # Then grab a new summary\n",
    "        if self.generate_new_metadata_ind:\n",
    "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the chunk new summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary']\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _update_chunk_title(self, chunk):\n",
    "        \"\"\"\n",
    "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
    "        \"\"\"\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good title will say what the chunk is about.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
    "\n",
    "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        updated_chunk_title = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary'],\n",
    "            \"current_title\" : chunk['title']\n",
    "        }).content\n",
    "\n",
    "        return updated_chunk_title\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the new chunk summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": proposition\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _get_new_chunk_title(self, summary):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "                    You will be given a summary of a chunk which needs a title\n",
    "\n",
    "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_title = runnable.invoke({\n",
    "            \"summary\": summary\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_title\n",
    "\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id' : new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title' : new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index' : len(self.chunks)\n",
    "        }\n",
    "        if self.print_logging:\n",
    "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
    "    \n",
    "    def get_chunk_outline(self):\n",
    "        \"\"\"\n",
    "        Get a string which represents the chunks you currently have.\n",
    "        This will be empty when you first start off\n",
    "        \"\"\"\n",
    "        chunk_outline = \"\"\n",
    "\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ID: {chunk['chunk_id']}\\nChunk Name: {chunk['title']}\\nChunk Summary: {chunk['summary']}\\n\\n\"\"\"\n",
    "        \n",
    "            chunk_outline += single_chunk_string\n",
    "        \n",
    "        return chunk_outline\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
    "\n",
    "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
    "                    The goal is to group similar propositions and chunks.\n",
    "\n",
    "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
    "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
    "\n",
    "                    Example:\n",
    "                    Input:\n",
    "                        - Proposition: \"Greg really likes hamburgers\"\n",
    "                        - Current Chunks:\n",
    "                            - Chunk ID: 2n4l3d\n",
    "                            - Chunk Name: Places in San Francisco\n",
    "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
    "\n",
    "                            - Chunk ID: 93833k\n",
    "                            - Chunk Name: Food Greg likes\n",
    "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
    "                    Output: 93833k\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
    "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        chunk_found = runnable.invoke({\n",
    "            \"proposition\": proposition,\n",
    "            \"current_chunk_outline\": current_chunk_outline\n",
    "        }).content\n",
    "\n",
    "        # Pydantic data class\n",
    "        class ChunkID(BaseModel):\n",
    "            \"\"\"Extracting the chunk id\"\"\"\n",
    "            chunk_id: Optional[str]\n",
    "            \n",
    "        # Extraction to catch-all LLM responses. This is a bandaid\n",
    "        structured_llm = self.llm.with_structured_output(ChunkID)\n",
    "        extraction_found = structured_llm.invoke(chunk_found)\n",
    "        if extraction_found:\n",
    "            chunk_found = extraction_found.chunk_id\n",
    "\n",
    "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
    "        # So return nothing\n",
    "        if chunk_found == None or len(chunk_found) != self.id_truncate_limit:\n",
    "            return None\n",
    "\n",
    "        return chunk_found\n",
    "    \n",
    "    def get_chunks(self, get_type='dict'):\n",
    "        \"\"\"\n",
    "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
    "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
    "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
    "        \"\"\"\n",
    "        if get_type == 'dict':\n",
    "            return self.chunks\n",
    "        if get_type == 'list_of_strings':\n",
    "            chunks = []\n",
    "            for chunk_id, chunk in self.chunks.items():\n",
    "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
    "            return chunks\n",
    "    \n",
    "    def pretty_print_chunks(self):\n",
    "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            print(f\"Summary: {chunk['summary']}\")\n",
    "            print(f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                print(f\"    -{prop}\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def pretty_print_chunk_outline(self):\n",
    "        print (\"Chunk Outline\\n\")\n",
    "        print(self.get_chunk_outline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ffb3f",
   "metadata": {},
   "source": [
    "# Clear Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ad1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def clean_graph(self):\n",
    "        with self.driver.session() as session:\n",
    "            session.execute_write(self._delete_all)\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_all(tx):\n",
    "        tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# Example usage\n",
    "conn = Neo4jConnection(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "conn.clean_graph()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaaf80",
   "metadata": {},
   "source": [
    "# Data Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfbdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "def load_high(datapath):\n",
    "    all_content = \"\"  # Initialize an empty string to hold all the content\n",
    "    with open(datapath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            all_content += line.strip() + \"\\n\"  # Append each line to the string, add newline character if needed\n",
    "    return all_content\n",
    "    \n",
    "# Pydantic data class\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "def get_propositions(text, runnable, structured_llm):\n",
    "    runnable_output = runnable.invoke({\n",
    "        \"input\": text\n",
    "    }).content\n",
    "    \n",
    "    propositions = structured_llm.invoke(runnable_output)\n",
    "    return propositions.sentences\n",
    "\n",
    "\n",
    "def run_chunk(essay):\n",
    "\n",
    "    obj = hub.pull(\"wfh/proposal-indexing\")\n",
    "    llm = ChatOpenAI(model=model, api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    runnable = obj | llm\n",
    "\n",
    "    # Extraction\n",
    "    structured_llm = llm.with_structured_output(Sentences)\n",
    "    \n",
    "    paragraphs = essay.split(\"\\n\\n\")\n",
    "\n",
    "    essay_propositions = []\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        propositions = get_propositions(para, runnable, structured_llm)\n",
    "        \n",
    "        essay_propositions.extend(propositions)\n",
    "        print (f\"Done with {i}\")\n",
    "\n",
    "    ac = AgenticChunker()\n",
    "    ac.add_propositions(essay_propositions)\n",
    "    ac.pretty_print_chunks()\n",
    "    chunks = ac.get_chunks(get_type='list_of_strings')\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca3c3d",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa569f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_metagraph(content, gid, n4j):\n",
    "\n",
    "    # Set instance\n",
    "    uio = UnstructuredIO()\n",
    "    kg_agent = KnowledgeGraphAgent()\n",
    "    whole_chunk = content\n",
    "\n",
    "    content = run_chunk(content)\n",
    "    for cont in content:\n",
    "        element_example = uio.create_element_from_text(text=cont)\n",
    "\n",
    "        ans_str = kg_agent.run(element_example, parse_graph_elements=False)\n",
    "        print(ans_str)\n",
    "\n",
    "        graph_elements = kg_agent.run(element_example, parse_graph_elements=True)\n",
    "        graph_elements = add_ge_emb(graph_elements)\n",
    "        graph_elements = add_gid(graph_elements, gid)\n",
    "\n",
    "        n4j.add_graph_elements(graph_elements=[graph_elements])\n",
    "    \n",
    "    merge_similar_nodes(n4j, gid)\n",
    "    add_sum(n4j, whole_chunk, gid)\n",
    "    return n4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c9af5",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47fd24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_p = \"\"\"\n",
    "Assess the similarity of the two provided summaries and return a rating from these options: 'very similar', 'similar', 'general', 'not similar', 'totally not similar'. Provide only the rating.\n",
    "\"\"\"\n",
    "\n",
    "def seq_ret(n4j, sumq):\n",
    "    rating_list = []\n",
    "    sumk = []\n",
    "    gids = []\n",
    "    sum_query = \"\"\"\n",
    "        MATCH (s:Summary)\n",
    "        RETURN s.content, s.gid\n",
    "        \"\"\"\n",
    "    res = n4j.query(sum_query)\n",
    "    for r in res:\n",
    "        sumk.append(r['s.content'])\n",
    "        gids.append(r['s.gid'])\n",
    "    \n",
    "    for sk in sumk:\n",
    "        sk = sk[0]\n",
    "        rate = ask_gpt(sys_p, \"The two summaries for comparison are: \\n Summary 1: \" + sk + \"\\n Summary 2: \" + sumq[0])\n",
    "        if \"totally not similar\" in rate:\n",
    "            rating_list.append(0)\n",
    "        elif \"not similar\" in rate:\n",
    "            rating_list.append(1)\n",
    "        elif \"general\" in rate:\n",
    "            rating_list.append(2)\n",
    "        elif \"very similar\" in rate:\n",
    "            rating_list.append(4)\n",
    "        elif \"similar\" in rate:\n",
    "            rating_list.append(3)\n",
    "        else:\n",
    "            print(\"llm returns no relevant rate\")\n",
    "            rating_list.append(-1)\n",
    "\n",
    "    ind = find_index_of_largest(rating_list)\n",
    "    gid = gids[ind]\n",
    "    \n",
    "    return gid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c19fe",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59d19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0\n",
      "\n",
      "Adding: 'The patient is an 82-year-old male with a history of coronary artery disease, previous myocardial infarction, and coronary artery bypass grafting.'\n",
      "No chunks, creating a new one\n",
      "Created new chunk (88510): Cardiovascular Patients\n",
      "\n",
      "Adding: 'He presented with symptoms including shakes at home while getting dressed, chest discomfort radiating to the shoulders, diaphoresis, and no chest pain at presentation.'\n",
      "No chunks found\n",
      "Created new chunk (a5333): Health Symptoms\n",
      "\n",
      "Adding: 'Electrocardiogram showed ST elevations in leads II, III, aVF, V3-V6, with reciprocal depressions in leads aVL, aVR, V1, and V2.'\n",
      "No chunks found\n",
      "Created new chunk (e8060): Heart Health\n",
      "\n",
      "Adding: 'Laboratory results indicated elevated cardiac enzymes, consistent with myocardial infarction.'\n",
      "Chunk Found (e8060), adding to: Heart Health\n",
      "\n",
      "Adding: 'Imaging confirmed a diagnosis of acute inferolateral myocardial infarction with three-vessel disease and occluded grafts.'\n",
      "Chunk Found (e8060), adding to: Cardiac Diagnostics\n",
      "\n",
      "Adding: 'The patient underwent emergent percutaneous coronary intervention with stent placement in the coronary arteries.'\n",
      "Chunk Found (e8060), adding to: Heart Attacks and Cardiac Conditions\n",
      "\n",
      "Adding: 'Post-procedure, the patient's chest pain improved, and he was started on antiplatelet and other cardiac medications.'\n",
      "Chunk Found (e8060), adding to: Cardiac Events\n",
      "\n",
      "Adding: 'Discharge medications included aspirin, clopidogrel, lisinopril, metoprolol, atorvastatin, and other supportive drugs.'\n",
      "No chunks found\n",
      "Created new chunk (00510): Medications and Prescriptions\n",
      "\n",
      "Adding: 'Follow-up appointments were scheduled with the cardiologist and primary care physician to monitor recovery and manage cardiovascular health.'\n",
      "Chunk Found (e8060), adding to: Myocardial Infarction Diagnostics and Treatment\n",
      "\n",
      "You have 4 chunks\n",
      "\n",
      "Chunk #0\n",
      "Chunk ID: 88510\n",
      "Summary: This chunk contains information about elderly patients with a history of cardiovascular diseases and related surgical interventions.\n",
      "Propositions:\n",
      "    -The patient is an 82-year-old male with a history of coronary artery disease, previous myocardial infarction, and coronary artery bypass grafting.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #1\n",
      "Chunk ID: a5333\n",
      "Summary: This chunk contains information about symptoms related to cardiovascular or health issues.\n",
      "Propositions:\n",
      "    -He presented with symptoms including shakes at home while getting dressed, chest discomfort radiating to the shoulders, diaphoresis, and no chest pain at presentation.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #2\n",
      "Chunk ID: e8060\n",
      "Summary: This chunk covers diagnostic findings and medical interventions related to heart attacks and cardiovascular health management.\n",
      "Propositions:\n",
      "    -Electrocardiogram showed ST elevations in leads II, III, aVF, V3-V6, with reciprocal depressions in leads aVL, aVR, V1, and V2.\n",
      "    -Laboratory results indicated elevated cardiac enzymes, consistent with myocardial infarction.\n",
      "    -Imaging confirmed a diagnosis of acute inferolateral myocardial infarction with three-vessel disease and occluded grafts.\n",
      "    -The patient underwent emergent percutaneous coronary intervention with stent placement in the coronary arteries.\n",
      "    -Post-procedure, the patient's chest pain improved, and he was started on antiplatelet and other cardiac medications.\n",
      "    -Follow-up appointments were scheduled with the cardiologist and primary care physician to monitor recovery and manage cardiovascular health.\n",
      "\n",
      "\n",
      "\n",
      "Chunk #3\n",
      "Chunk ID: 00510\n",
      "Summary: This chunk contains information about medications prescribed upon discharge, including common cardiovascular and supportive drugs.\n",
      "Propositions:\n",
      "    -Discharge medications included aspirin, clopidogrel, lisinopril, metoprolol, atorvastatin, and other supportive drugs.\n",
      "\n",
      "\n",
      "\n",
      "Nodes:\n",
      "\n",
      "Node(id='82-year-old male', type='Person')\n",
      "Node(id='coronary artery disease', type='MedicalCondition')\n",
      "Node(id='previous myocardial infarction', type='MedicalCondition')\n",
      "Node(id='coronary artery bypass grafting', type='MedicalProcedure')\n",
      "\n",
      "Relationships:\n",
      "\n",
      "Relationship(subj=Node(id='82-year-old male', type='Person'), obj=Node(id='coronary artery disease', type='MedicalCondition'), type='HasCondition')\n",
      "Relationship(subj=Node(id='82-year-old male', type='Person'), obj=Node(id='previous myocardial infarction', type='MedicalCondition'), type='HasCondition')\n",
      "Relationship(subj=Node(id='82-year-old male', type='Person'), obj=Node(id='coronary artery bypass grafting', type='MedicalProcedure'), type='UnderwentProcedure')\n",
      "Nodes:\n",
      "\n",
      "Node(id='Symptoms', type='MedicalCondition')\n",
      "Node(id='Shakes', type='Symptom')\n",
      "Node(id='Chest Discomfort', type='Symptom')\n",
      "Node(id='Shoulders', type='BodyPart')\n",
      "Node(id='Diaphoresis', type='Symptom')\n",
      "Node(id='Chest Pain', type='Symptom')\n",
      "Node(id='Presentation', type='Event')\n",
      "\n",
      "Relationships:\n",
      "\n",
      "Relationship(subj=Node(id='Symptoms', type='MedicalCondition'), obj=Node(id='Shakes', type='Symptom'), type='Includes')\n",
      "Relationship(subj=Node(id='Symptoms', type='MedicalCondition'), obj=Node(id='Chest Discomfort', type='Symptom'), type='Includes')\n",
      "Relationship(subj=Node(id='Chest Discomfort', type='Symptom'), obj=Node(id='Shoulders', type='BodyPart'), type='RadiatesTo')\n",
      "Relationship(subj=Node(id='Symptoms', type='MedicalCondition'), obj=Node(id='Diaphoresis', type='Symptom'), type='Includes')\n",
      "Relationship(subj=Node(id='Symptoms', type='MedicalCondition'), obj=Node(id='Chest Pain', type='Symptom'), type='Excludes')\n",
      "Relationship(subj=Node(id='Symptoms', type='MedicalCondition'), obj=Node(id='Presentation', type='Event'), type='DescribedAt')\n",
      "Based on the provided content, here are the extracted nodes and relationships structured into the specified format:\n",
      "\n",
      "### Nodes:\n",
      "\n",
      "- Node(id='Electrocardiogram', type='Test')\n",
      "- Node(id='ST elevations', type='MedicalFinding')\n",
      "- Node(id='leads II', type='Lead')\n",
      "- Node(id='leads III', type='Lead')\n",
      "- Node(id='leads aVF', type='Lead')\n",
      "- Node(id='leads V3-V6', type='Lead')\n",
      "- Node(id='reciprocal depressions', type='MedicalFinding')\n",
      "- Node(id='leads aVL', type='Lead')\n",
      "- Node(id='leads aVR', type='Lead')\n",
      "- Node(id='leads V1', type='Lead')\n",
      "- Node(id='leads V2', type='Lead')\n",
      "- Node(id='Laboratory results', type='Test')\n",
      "- Node(id='cardiac enzymes', type='Biomarker')\n",
      "- Node(id='myocardial infarction', type='Condition')\n",
      "- Node(id='acute inferolateral myocardial infarction', type='Condition')\n",
      "- Node(id='three-vessel disease', type='Condition')\n",
      "- Node(id='occluded grafts', type='Condition')\n",
      "- Node(id='patient', type='Person')\n",
      "- Node(id='percutaneous coronary intervention', type='Procedure')\n",
      "- Node(id='stent placement', type='Procedure')\n",
      "- Node(id='coronary arteries', type='Anatomy')\n",
      "- Node(id='chest pain', type='Symptom')\n",
      "- Node(id='antiplatelet medications', type='Medication')\n",
      "- Node(id='cardiac medications', type='Medication')\n",
      "- Node(id='cardiologist', type='HealthcareProvider')\n",
      "- Node(id='primary care physician', type='HealthcareProvider')\n",
      "- Node(id='cardiovascular health', type='HealthAspect')\n",
      "\n",
      "### Relationships:\n",
      "\n",
      "- Relationship(subj=Node(id='Electrocardiogram', type='Test'), obj=Node(id='ST elevations', type='MedicalFinding'), type='Shows')\n",
      "- Relationship(subj=Node(id='Electrocardiogram', type='Test'), obj=Node(id='reciprocal depressions', type='MedicalFinding'), type='Shows')\n",
      "- Relationship(subj=Node(id='Laboratory results', type='Test'), obj=Node(id='cardiac enzymes', type='Biomarker'), type='Indicates')\n",
      "- Relationship(subj=Node(id='Laboratory results', type='Test'), obj=Node(id='myocardial infarction', type='Condition'), type='ConsistentWith')\n",
      "- Relationship(subj=Node(id='Imaging', type='Test'), obj=Node(id='acute inferolateral myocardial infarction', type='Condition'), type='ConfirmsDiagnosis')\n",
      "- Relationship(subj=Node(id='acute inferolateral myocardial infarction', type='Condition'), obj=Node(id='three-vessel disease', type='Condition'), type='AssociatedWith')\n",
      "- Relationship(subj=Node(id='acute inferolateral myocardial infarction', type='Condition'), obj=Node(id='occluded grafts', type='Condition'), type='AssociatedWith')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='percutaneous coronary intervention', type='Procedure'), type='Underwent')\n",
      "- Relationship(subj=Node(id='percutaneous coronary intervention', type='Procedure'), obj=Node(id='stent placement', type='Procedure'), type='Includes')\n",
      "- Relationship(subj=Node(id='stent placement', type='Procedure'), obj=Node(id='coronary arteries', type='Anatomy'), type='Involves')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='chest pain', type='Symptom'), type='Experiences')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='antiplatelet medications', type='Medication'), type='StartedOn')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='cardiac medications', type='Medication'), type='StartedOn')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='cardiologist', type='HealthcareProvider'), type='ScheduledFollowUpWith')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='primary care physician', type='HealthcareProvider'), type='ScheduledFollowUpWith')\n",
      "- Relationship(subj=Node(id='patient', type='Person'), obj=Node(id='cardiovascular health', type='HealthAspect'), type='Manage')\n",
      "Nodes:\n",
      "\n",
      "Node(id='Aspirin', type='Medication')\n",
      "Node(id='Clopidogrel', type='Medication')\n",
      "Node(id='Lisinopril', type='Medication')\n",
      "Node(id='Metoprolol', type='Medication')\n",
      "Node(id='Atorvastatin', type='Medication')\n",
      "Node(id='Supportive Drugs', type='Medication')\n",
      "\n",
      "Relationships:\n",
      "\n",
      "Relationship(subj=Node(id='Aspirin', type='Medication'), obj=Node(id='Supportive Drugs', type='Medication'), type='IncludedInDischarge')\n",
      "Relationship(subj=Node(id='Clopidogrel', type='Medication'), obj=Node(id='Supportive Drugs', type='Medication'), type='IncludedInDischarge')\n",
      "Relationship(subj=Node(id='Lisinopril', type='Medication'), obj=Node(id='Supportive Drugs', type='Medication'), type='IncludedInDischarge')\n",
      "Relationship(subj=Node(id='Metoprolol', type='Medication'), obj=Node(id='Supportive Drugs', type='Medication'), type='IncludedInDischarge')\n",
      "Relationship(subj=Node(id='Atorvastatin', type='Medication'), obj=Node(id='Supportive Drugs', type='Medication'), type='IncludedInDischarge')\n",
      "2025-05-08 08:28:38,512 - neo4j.notifications - WARNING - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: REFERENCE)} {position: line: 7, column: 22, offset: 261} for query: '\\n        // Match all \\'n\\' nodes with a specific gid but not of the \"Summary\" type\\n        MATCH (n)\\n        WHERE n.gid = $gid AND NOT n:Summary\\n\\n        // Find all \\'m\\' nodes where \\'m\\' is a reference of \\'n\\' via a \\'REFERENCES\\' relationship\\n        MATCH (n)-[r:REFERENCE]->(m)\\n        WHERE NOT m:Summary\\n\\n        // Find all \\'o\\' nodes connected to each \\'m\\', and include the relationship type,\\n        // while excluding \\'Summary\\' type nodes and \\'REFERENCE\\' relationship\\n        MATCH (m)-[s]-(o)\\n        WHERE NOT o:Summary AND TYPE(s) <> \\'REFERENCE\\'\\n\\n        // Collect and return details in a structured format\\n        RETURN n.id AS NodeId1, \\n            m.id AS Mid, \\n            TYPE(r) AS ReferenceType, \\n            collect(DISTINCT {RelationType: type(s), Oid: o.id}) AS Connections\\n    '\n",
      "Done with 0\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of left hemisphere stroke.'\n",
      "No chunks, creating a new one\n",
      "Created new chunk (57951): Personal Medical History\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of atrial fibrillation.'\n",
      "Chunk Found (57951), adding to: Personal Medical History\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of psychotic disorder.'\n",
      "Chunk Found (57951), adding to: Medical History and Cardiovascular Health\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of type II diabetes mellitus.'\n",
      "Chunk Found (57951), adding to: Medical History and Mental Health\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of hypertension.'\n",
      "Chunk Found (57951), adding to: Medical Conditions and Demographics\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of left middle cerebral artery aneurysm.'\n",
      "Chunk Found (57951), adding to: Medical History and Demographics\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with a history of subarachnoid hemorrhage (SAH) due to aneurysm rupture.'\n",
      "Chunk Found (57951), adding to: Medical Conditions and Demographics\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male who was previously treated with aneurysmal clipping via left frontotemporal craniotomy.'\n",
      "Chunk Found (57951), adding to: Medical History\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male who previously had a large blood clot removed from the left frontotemporal area of his brain during craniotomy.'\n",
      "Chunk Found (57951), adding to: Medical Conditions\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male who has post-stroke epilepsy managed on dilantin (phenytoin) monotherapy.'\n",
      "Chunk Found (57951), adding to: Medical History\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male who presents today as a code stroke.'\n",
      "Chunk Found (57951), adding to: Medical Conditions\n",
      "\n",
      "Adding: 'Mr. is a 61-year-old male with complaints of right-sided weakness and slurred speech.'\n",
      "Chunk Found (57951), adding to: Medical History\n",
      "\n",
      "Adding: 'Mr. was found by his wife lying on the couch with his right arm and leg hanging over the couch.'\n",
      "No chunks found\n",
      "Created new chunk (c003a): Physical State & Location\n",
      "\n",
      "Adding: 'Mr. was unable to move his right arm voluntarily.'\n",
      "Chunk Found (c003a), adding to: Physical State & Location\n",
      "\n",
      "Adding: 'Mr. was complaining 'I can't breathe'.'\n",
      "Chunk Found (c003a), adding to: Mobility and Physical Condition\n",
      "\n",
      "Adding: 'Mr. was brought to the emergency department by emergency medical services.'\n",
      "Chunk Found (c003a), adding to: Health Emergency: Mobility and Breathing\n",
      "\n",
      "Adding: 'Upon arrival to the emergency department, Mr.'s fingerstick blood glucose was 209.'\n",
      "Chunk Found (c003a), adding to: Medical Emergencies\n",
      "\n",
      "Adding: 'Review of systems could not be obtained directly from Mr. due to his distress and dysarthria.'\n",
      "No chunks found\n",
      "Created new chunk (c9689): Medical History Challenges\n",
      "\n",
      "Adding: 'Mr. was given tissue plasminogen activator (tPA) for stroke treatment.'\n",
      "Chunk Found (c003a), adding to: Health Emergencies\n",
      "\n",
      "Adding: 'Mr. was admitted to the neuro intensive care unit after stroke treatment.'\n",
      "Chunk Found (c003a), adding to: Medical Emergencies\n",
      "\n",
      "Adding: 'The wife provided a patchy history of Mr.'s medical background.'\n",
      "No chunks found\n",
      "Created new chunk (c0114): Health & Medical History\n",
      "\n",
      "Adding: 'Mr. is normally quite independent at baseline.'\n",
      "No chunks found\n",
      "Created new chunk (784db): Personal Independence\n",
      "\n",
      "Adding: 'Mr. enjoys watching TV as his usual activity.'\n",
      "No chunks found\n",
      "Created new chunk (14489): Leisure Activities\n",
      "\n",
      "Adding: 'Mr. can ambulate without difficulties at baseline.'\n",
      "No chunks found\n",
      "Created new chunk (d8d62): Mobility & Physical Ability\n",
      "\n",
      "Adding: 'Mr. has no baseline speech or language deficits.'\n",
      "No chunks found\n",
      "Created new chunk (f4451): Speech and Language Skills\n",
      "\n",
      "Adding: 'Recently, Mr. has experienced generalized weakness due to fatigue.'\n",
      "No chunks found\n",
      "Created new chunk (59f57): Health and Wellness\n",
      "\n",
      "Adding: 'Mr. has been compliant with his medications.'\n",
      "No chunks found\n",
      "Created new chunk (787ff): Medication Adherence\n",
      "\n",
      "Adding: 'Mr. was in his usual state of health until approximately 8 pm on the day of presentation.'\n",
      "No chunks found\n",
      "Created new chunk (b4f17): Health and Wellness\n",
      "\n",
      "Adding: 'His wife was with him watching TV until about 8 pm.'\n",
      "Chunk Found (14489), adding to: Leisure Activities\n",
      "\n",
      "Adding: 'His wife briefly stepped away to use the bathroom.'\n",
      "No chunks found\n",
      "Created new chunk (e4f86): Personal Activities & Routines\n",
      "\n",
      "Adding: 'When his wife returned, she found Mr. lying on the couch with his right arm and leg hanging over the couch.'\n",
      "No chunks found\n",
      "Created new chunk (97c11): Physical State & Location\n",
      "\n",
      "Adding: 'When his wife returned, Mr. was unable to move his right arm voluntarily.'\n",
      "No chunks found\n",
      "Created new chunk (1590f): Medical Conditions\n",
      "\n",
      "Adding: 'When his wife returned, Mr. was complaining 'I can't breathe'.'\n",
      "No chunks found\n",
      "Created new chunk (fc5b9): Respiratory Health\n",
      "\n",
      "Adding: 'His wife immediately called 911.'\n",
      "No chunks found\n",
      "Created new chunk (5ff9f): Emergency Response\n",
      "\n",
      "Adding: 'In the emergency department, Mr.'s fingerstick blood sugar was 209.'\n",
      "No chunks found\n",
      "Created new chunk (0a28a): Medical Emergencies and Blood Sugar\n",
      "\n",
      "Adding: 'A review of systems was attempted but could not be obtained from Mr. himself.'\n",
      "No chunks found\n",
      "Created new chunk (8c632): Healthcare Data Collection\n",
      "\n",
      "Adding: 'Mr. was significantly dysarthric and in distress.'\n",
      "Chunk Found (f4451), adding to: Speech and Language Skills\n",
      "\n",
      "Adding: 'Mr. was administered tissue plasminogen activator (tPA).'\n",
      "Chunk Found (c003a), adding to: Stroke and Neurological Emergencies\n",
      "\n",
      "Adding: 'Details of tPA administration can be found in the stroke fellow's note.'\n",
      "Chunk Found (c003a), adding to: Neurological Emergencies\n",
      "\n",
      "Adding: 'Mr. was admitted to the neuro ICU after stroke treatment.'\n",
      "Chunk Found (c003a), adding to: Stroke and Neurological Emergency Management\n",
      "\n",
      "Adding: 'Historical mention: Mr. has a psychotic disorder not otherwise specified (NOS).'\n",
      "No chunks found\n",
      "Created new chunk (1022a): Historical Mental Health References\n",
      "\n",
      "Adding: 'Psychotic disorder NOS was briefly noted on discharge summary from an unspecified hospital.'\n",
      "Chunk Found (1022a), adding to: Historical Mental Health References\n",
      "\n",
      "Adding: 'Mr. is currently on a low dose of fluphenazine.'\n",
      "Chunk Found (1022a), adding to: Mental Health History\n",
      "\n",
      "Adding: 'There was difficulty obtaining further history from Mr.'s wife regarding the psychotic disorder.'\n",
      "Chunk Found (e4f86), adding to: Personal Activities & Routines\n",
      "\n",
      "Adding: 'His wife was unaware of the exact diagnosis of psychotic disorder NOS.'\n",
      "No chunks found\n",
      "Created new chunk (18e9f): Health & Medical Awareness\n",
      "\n",
      "Adding: 'Mr. has a history of atrial fibrillation that has been noted in the past.'\n",
      "Chunk Found (c003a), adding to: Neurological Emergencies\n",
      "\n",
      "Adding: 'Mr. was not on Coumadin (warfarin) for atrial fibrillation due to fall risk.'\n",
      "Chunk Found (787ff), adding to: Medication Adherence\n",
      "\n",
      "Adding: 'Mr. had a history of left MCA aneurysm rupture resulting in subarachnoid hemorrhage.'\n",
      "Chunk Found (e4f86), adding to: Medical History Challenges\n",
      "\n",
      "Adding: 'The aneurysm was clipped, performed by a doctor at an unspecified hospital.'\n",
      "Chunk Found (e4f86), adding to: Neurological History and Personal Activities\n",
      "\n",
      "Adding: 'The operation involved a left frontotemporal craniotomy approach.'\n",
      "No chunks found\n",
      "Created new chunk (2041a): Medical Procedures\n",
      "\n",
      "Adding: 'A large amount of blood clot was removed during craniotomy in the affected area.'\n",
      "Chunk Found (2041a), adding to: Medical Procedures\n",
      "\n",
      "Adding: 'Mr. had post-stroke seizures and was admitted to neuromedicine.'\n",
      "Chunk Found (c003a), adding to: Stroke and Neurological Emergency Management\n",
      "\n",
      "Adding: 'Mr. presented with aphasia and right hemiparesis during this prior hospitalization.'\n",
      "Chunk Found (c9689), adding to: Medical History Challenges\n",
      "\n",
      "Adding: 'Initial CT scan showed significant frontotemporal encephalomalacia.'\n",
      "No chunks found\n",
      "Created new chunk (f9e69): Brain Injuries & Conditions\n",
      "\n",
      "Adding: 'Mr. had an LP (lumbar puncture) which was unremarkable.'\n",
      "No chunks found\n",
      "Created new chunk (c0630): Medical Procedures and Diagnostics\n",
      "\n",
      "Adding: 'Mr. had EEG monitoring showing bursts of semi-rhythmic 2 to 4 Hz activity involving the left hemisphere.'\n",
      "Chunk Found (c9689), adding to: Neurological Symptoms\n",
      "\n",
      "Adding: 'EEG also showed focal slowing and occasional sharp wave discharges.'\n",
      "Chunk Found (c9689), adding to: Neurological Conditions\n",
      "\n",
      "Adding: 'Mr. was started on dilantin (phenytoin) therapy after EEG findings.'\n",
      "Chunk Found (c9689), adding to: Neurological Symptoms and Brain Activity\n",
      "\n",
      "Adding: 'Mr.'s symptoms improved after initiation of anticonvulsant therapy.'\n",
      "No chunks found\n",
      "Created new chunk (4e135): Medical Treatments and Effects\n",
      "\n",
      "Adding: 'Mr. has seizure episodes approximately once per year, typically involving loss of consciousness with shaking of limbs.'\n",
      "No chunks found\n",
      "Created new chunk (250ac): Seizure Episodes\n",
      "\n",
      "Adding: 'Mr. has a history of hypertension.'\n",
      "No chunks found\n",
      "Created new chunk (a82c6): Blood Pressure & Health Conditions\n",
      "\n",
      "Adding: 'Mr. is unemployed since his aneurysm rupture.'\n",
      "No chunks found\n",
      "Created new chunk (75a3f): Health and Employment\n",
      "\n",
      "Adding: 'Mr. quit smoking in his early 20s, smoking about 1 pack per day.'\n",
      "No chunks found\n",
      "Created new chunk (3fd7d): Health & Lifestyle\n",
      "\n",
      "Adding: 'He walks at home with a cane.'\n",
      "No chunks found\n",
      "Created new chunk (8cb20): Home Mobility Aids\n",
      "\n",
      "Adding: 'He occasionally uses a bath seat for showering at home.'\n",
      "No chunks found\n",
      "Created new chunk (414cb): Personal Hygiene Aids\n",
      "\n",
      "Adding: 'Mr. does not consume alcohol or illicit drugs.'\n",
      "No chunks found\n",
      "Created new chunk (f874c): Substance Use and Lifestyle\n",
      "\n",
      "Adding: 'He has three grown children.'\n",
      "No chunks found\n",
      "Created new chunk (1f4fe): Family & Relationships\n",
      "\n",
      "Adding: 'Family history is negative for seizures and strokes.'\n",
      "Chunk Found (1f4fe), adding to: Family & Relationships\n",
      "\n",
      "Adding: 'Physical exam on admission shows Mr. is awake, cooperative, in mild distress.'\n",
      "No chunks found\n",
      "Created new chunk (44149): Patient Assessments\n",
      "\n",
      "Adding: 'Mr. intermittently stares blankly and may laugh inappropriately.'\n",
      "Chunk Found (f4451), adding to: Speech and Communication Impairments\n",
      "\n",
      "Adding: 'Heuss: no scleral icterus, moist mucous membranes, no lesions in oropharynx.'\n",
      "No chunks found\n",
      "Created new chunk (3f96e): Health and Medical Observations\n",
      "\n",
      "Adding: 'Neck: supple, no masses or lymphadenopathy.'\n",
      "No chunks found\n",
      "Created new chunk (7a8e8): Physical Examination Findings\n",
      "\n",
      "Adding: 'Lungs: bilateral clear to auscultation, no rales, rhonchi, or wheezes.'\n",
      "No chunks found\n",
      "Created new chunk (48280): Respiratory Health\n",
      "\n",
      "Adding: 'Heart: regular rate and rhythm, normal S1 and S2, no murmurs, rubs, or gallops.'\n",
      "Chunk Found (a82c6), adding to: Blood Pressure & Health Conditions\n",
      "\n",
      "Adding: 'Abdomen: obese, soft, non-tender, non-distended, no masses or organomegaly.'\n",
      "No chunks found\n",
      "Created new chunk (3ef78): Medical Examination Findings\n",
      "\n",
      "Adding: 'Extremities: warm, well-perfused, poor nail hygiene.'\n",
      "No chunks found\n",
      "Created new chunk (2fc1c): Circulation & Hygiene Conditions\n",
      "\n",
      "Adding: 'Skin: no rashes or lesions noted.'\n",
      "No chunks found\n",
      "Created new chunk (a9afa): Health and Medical Conditions\n",
      "\n",
      "Adding: 'Neurologic exam: Mr. is awake, alert, and makes good eye contact.'\n",
      "No chunks found\n",
      "Created new chunk (85a69): Neurological Assessments\n",
      "\n",
      "Adding: 'Mr. is significantly dysarthric.'\n",
      "Chunk Found (f4451), adding to: Neurological and Emotional Symptoms\n",
      "\n",
      "Adding: 'Mr. can tell his name, the current month, and his wife's name.'\n",
      "No chunks found\n",
      "Created new chunk (23121): Personal Identification & Basic Info\n",
      "\n",
      "Adding: 'Mr. reports that he is in an unspecified month (month missing).'\n",
      "No chunks found\n",
      "Created new chunk (5f167): Date & Times\n",
      "\n",
      "Adding: 'Mr. follows simple midline commands.'\n",
      "Chunk Found (85a69), adding to: Neurological Assessments\n",
      "\n",
      "Adding: 'Mr. stares blankly at times.'\n",
      "Chunk Found (f4451), adding to: Speech and Neurological Behaviors\n",
      "\n",
      "Adding: 'Mr. is difficult to test formally due to distress.'\n",
      "No chunks found\n",
      "Created new chunk (f99b4): Mental Health Challenges\n",
      "\n",
      "Adding: 'Mr. comprehends well and repeats speech well, with significant dysarthria.'\n",
      "Chunk Found (f4451), adding to: Speech and Neurological Signs\n",
      "\n",
      "Adding: 'Cranial nerve I: not tested.'\n",
      "Chunk Found (7a8e8), adding to: Physical Examination Findings\n",
      "\n",
      "Adding: 'Cranial nerve II: pupils are 3 to 2 mm and briskly reactive to light.'\n",
      "No chunks found\n",
      "Created new chunk (3edce): Neurological Assessments\n",
      "\n",
      "Adding: 'Visual field testing was limited; possible right homonymous hemianopsia noted.'\n",
      "Chunk Found (7a8e8), adding to: Head, Neck & Cranial Nerve Exam\n",
      "\n",
      "Adding: 'Cranial nerves III, IV, VI: extraocular movements are intact; prefers left gaze.'\n",
      "No chunks found\n",
      "Created new chunk (8cefc): Eye Movement Control\n",
      "\n",
      "Adding: 'Cranial nerve V: facial sensation is intact to light touch.'\n",
      "No chunks found\n",
      "Created new chunk (d949b): Facial Nerve Function\n",
      "\n",
      "Adding: 'Cranial nerve VII: right facial droop involving the forehead.'\n",
      "Chunk Found (d949b), adding to: Facial Nerve Function\n",
      "\n",
      "Adding: 'Cranial nerve VIII: hearing grossly intact bilaterally.'\n",
      "Chunk Found (8cefc), adding to: Eye Movement Control\n",
      "\n",
      "Adding: 'Cranial nerves IX, X: palate appears midline; difficult to test specifically.'\n",
      "Chunk Found (8cefc), adding to: Cranial Nerves and Sensory Functions\n",
      "\n",
      "Adding: 'Cranial nerve XII: tongue protrudes in midline with good movements in all directions.'\n",
      "Chunk Found (8cefc), adding to: Cranial Nerves and Facial Movements\n",
      "\n",
      "Adding: 'Motor exam: normal bulk and tone in extremities.'\n",
      "No chunks found\n",
      "Created new chunk (c81e3): Motor Function & Muscles\n",
      "\n",
      "Adding: 'Left arm and leg: antigravity and withdraw to pain.'\n",
      "Chunk Found (c9689), adding to: Neurological Assessments\n",
      "\n",
      "Adding: 'Right arm: plegic below the deltoid and distally.'\n",
      "Chunk Found (c9689), adding to: Neurological Symptoms and Brain Activity\n",
      "\n",
      "Adding: 'Right leg: antigravity, with frequent kicking to maintain upright position.'\n",
      "No chunks found\n",
      "Created new chunk (8b6a9): Motor Skills and Posture\n",
      "\n",
      "Adding: 'Sensory exam: noxious stimuli sensed in all four extremities.'\n",
      "No chunks found\n",
      "Created new chunk (fcbd4): Sensory Responses to Noxious Stimuli\n",
      "\n",
      "Adding: 'Hard to formalize sensory testing; sensation to light touch present in all extremities.'\n",
      "No chunks found\n",
      "Created new chunk (163c6): Sensory Testing and Perception\n",
      "\n",
      "Adding: 'Deep tendon reflexes: hyporeflexic bilaterally.'\n",
      "No chunks found\n",
      "Created new chunk (afad6): Neurological Reflexes\n",
      "\n",
      "Adding: 'Plantar response: downgoing on the left, upgoing on the right.'\n",
      "Chunk Found (c81e3), adding to: Motor Function & Muscles\n",
      "\n",
      "Adding: 'Coordination: finger to nose test intact on the left; not testable on the right.'\n",
      "No chunks found\n",
      "Created new chunk (389e4): Neurological Tests\n",
      "\n",
      "Adding: 'Gait: not tested.'\n",
      "No chunks found\n",
      "Created new chunk (e46f9): Gait Analysis Planning\n",
      "\n",
      "Adding: 'Summary of pertinent findings: ECG shows atrial fibrillation with controlled ventricular response.'\n",
      "No chunks found\n",
      "Created new chunk (7688c): Cardiac Health & Arrhythmias\n",
      "\n",
      "Adding: 'ECG: delayed R wave transition in anterior precordial leads.'\n",
      "No chunks found\n",
      "Created new chunk (6c43c): Heart Conduction & ECG Findings\n",
      "\n",
      "Adding: 'ECG: non-specific ST-T wave changes in inferior and anterolateral leads.'\n",
      "Chunk Found (6c43c), adding to: Heart Conduction & ECG Findings\n",
      "\n",
      "Adding: 'Previous ECG unavailable for comparison.'\n",
      "Chunk Found (6c43c), adding to: Cardiac Electrical Activity\n",
      "\n",
      "Adding: 'CTA head: no acute intracranial abnormality identified.'\n",
      "No chunks found\n",
      "Created new chunk (a72bb): Medical Imaging Results\n",
      "\n",
      "Adding: 'CTA head: encephalomalacia in the left MCA distribution with ex vacuo ventricular dilatation.'\n",
      "Chunk Found (a72bb), adding to: Medical Imaging Results\n",
      "\n",
      "Adding: 'CTA head: possible hyperdensity in the left MCA suggestive of hyperdense MCA sign; CTA images are suboptimal.'\n",
      "Chunk Found (3f96e), adding to: Health and Medical Observations\n",
      "\n",
      "Adding: 'CTA head: unremarkable cerebral perfusion study.'\n",
      "Chunk Found (3f96e), adding to: Brain Imaging and Stroke Signs\n",
      "\n",
      "Adding: 'EEG: evidence of nearly continuous left temporal epileptiform discharges, sometimes periodic, indicating potential epileptogenic focus.'\n",
      "Chunk Found (250ac), adding to: Seizure Episodes\n",
      "\n",
      "Adding: 'EEG shows diffuse cerebral slowing, suggestive of mild to moderate encephalopathy.'\n",
      "Chunk Found (3f96e), adding to: Neurological Imaging and Assessments\n",
      "\n",
      "Adding: 'EEG background activity more slowed over the left hemisphere, indicating cortical and subcortical dysfunction.'\n",
      "Chunk Found (3f96e), adding to: Neurological Diagnostics\n",
      "\n",
      "Adding: 'EEG notes sinus tachycardia.'\n",
      "No chunks found\n",
      "Created new chunk (50aed): Medical Observations\n",
      "\n",
      "Adding: 'Transthoracic echocardiogram (TTE): no clot in the left atrium.'\n",
      "Chunk Found (7688c), adding to: Cardiac Health & Arrhythmias\n",
      "\n",
      "Adding: 'TTE: markedly dilated right atrium with no atrial septal defect.'\n",
      "Chunk Found (6c43c), adding to: Electrocardiogram Findings\n",
      "\n",
      "Adding: 'TTE: mild symmetric left ventricular hypertrophy.'\n",
      "No chunks found\n",
      "Created new chunk (95a6d): Heart Conditions\n",
      "\n",
      "Adding: 'LVEF (left ventricular ejection fraction): low normal at 50-55%.'\n",
      "Chunk Found (7688c), adding to: Cardiac Diagnostics & Arrhythmias\n",
      "\n",
      "Adding: 'A posterior aortic root abnormality noted; cannot rule out aortic dissection.'\n",
      "Chunk Found (7688c), adding to: Cardiac Health & Diagnostics\n",
      "\n",
      "Adding: 'NCHCT: no evidence of hemorrhage.'\n",
      "Chunk Found (3f96e), adding to: Brain Imaging and Neurological Assessments\n",
      "\n",
      "Adding: 'NCHCT: stable encephalomalacia in left MCA territory.'\n",
      "Chunk Found (2041a), adding to: Neurosurgical Procedures\n",
      "\n",
      "Adding: 'NCHCT: evolving infarct in the posterior part of the left MCA territory.'\n",
      "Chunk Found (7a8e8), adding to: Neurological and Physical Head & Eye Exam\n",
      "\n",
      "Adding: 'NCHCT: unchanged cystic encephalomalacia with ex vacuo ventricular dilation from prior infarct.'\n",
      "Chunk Found (2041a), adding to: Brain Surgery and Conditions\n",
      "\n",
      "Adding: 'NCHCT: high-grade stenosis of the proximal right internal carotid artery (ICA).'\n",
      "Chunk Found (c9689), adding to: Neurological Assessments\n",
      "\n",
      "Adding: 'NCHCT: no hemorrhagic transformation of infarct.'\n",
      "Chunk Found (3f96e), adding to: Neurological Imaging and Function\n",
      "\n",
      "Adding: 'MRI: large diffusion abnormality in left anterior and middle cerebral artery territories indicating acute infarct.'\n",
      "Chunk Found (3f96e), adding to: Brain Imaging and Neurological Assessments\n",
      "\n",
      "Adding: 'MRI: scattered foci of blood products or mineralization may relate to prior hemorrhage; not considered dense enough for hemorrhagic transformation.'\n",
      "No chunks found\n",
      "Created new chunk (0da77): Medical Imaging and Hemorrhage\n",
      "\n",
      "Adding: 'MRI: moderate edema and mild rightward shift approximately 3–4mm.'\n",
      "Chunk Found (3f96e), adding to: Neurological Imaging and Stroke Evaluation\n",
      "\n",
      "Adding: 'MRI: better extent delineation than prior CT studies.'\n",
      "Chunk Found (3f96e), adding to: Brain Imaging and Stroke Assessment\n",
      "\n",
      "Adding: 'Focal areas of negative susceptibility within acute infarct area possibly due to blood products or mineralization.'\n",
      "Chunk Found (3f96e), adding to: Neurological Imaging and Stroke Evaluation\n",
      "\n",
      "Adding: 'Digital EEG shows worsening of epileptiform activity compared to previous day, with more discharges in the left temporal region and occasional discharges on the right.'\n",
      "Chunk Found (250ac), adding to: Epilepsy and Seizure Activity\n",
      "\n",
      "Adding: 'Potential causes of EEG worsening include metabolic abnormalities or structural lesions.'\n",
      "No chunks found\n",
      "Created new chunk (ec279): Medical Factors Affecting Brain Function\n",
      "\n",
      "Adding: 'Follow-up with clinical correlation is advised.'\n",
      "No chunks found\n",
      "Created new chunk (20556): Medical Follow-up & Assessments\n",
      "\n",
      "Adding: 'CTA head: evidence of evolving infarct in the superior division of the left MCA, unchanged from prior exam, with no hemorrhage.'\n",
      "No chunks found\n",
      "Created new chunk (0f187): Brain Imaging and Stroke\n",
      "\n",
      "Adding: 'Chronic infarction with encephalomalacia and ex vacuo dilatation in the left MCA territory.'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'c96e8'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m     content = load_high(file_path)\n\u001b[32m     15\u001b[39m     gid = str_uuid()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     n4j = \u001b[43mcreat_metagraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn4j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     link_context(n4j, \u001b[33m'\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m merge_similar_nodes(n4j, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcreat_metagraph\u001b[39m\u001b[34m(content, gid, n4j)\u001b[39m\n\u001b[32m      5\u001b[39m kg_agent = KnowledgeGraphAgent()\n\u001b[32m      6\u001b[39m whole_chunk = content\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m content = \u001b[43mrun_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cont \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[32m     10\u001b[39m     element_example = uio.create_element_from_text(text=cont)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mrun_chunk\u001b[39m\u001b[34m(essay)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m ac = AgenticChunker()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mac\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_propositions\u001b[49m\u001b[43m(\u001b[49m\u001b[43messay_propositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m ac.pretty_print_chunks()\n\u001b[32m     45\u001b[39m chunks = ac.get_chunks(get_type=\u001b[33m'\u001b[39m\u001b[33mlist_of_strings\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mAgenticChunker.add_propositions\u001b[39m\u001b[34m(self, propositions)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_propositions\u001b[39m(\u001b[38;5;28mself\u001b[39m, propositions):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m proposition \u001b[38;5;129;01min\u001b[39;00m propositions:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_proposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposition\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mAgenticChunker.add_proposition\u001b[39m\u001b[34m(self, proposition)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_id:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_logging:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChunk Found (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_id\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), adding to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.chunks[chunk_id][\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_proposition_to_chunk(chunk_id, proposition)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'c96e8'"
     ]
    }
   ],
   "source": [
    "# Set Neo4j instance\n",
    "n4j = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "data_path = \"./dataset\"\n",
    "files = [file for file in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, file))]\n",
    "\n",
    "# Read and print the contents of each file\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    content = load_high(file_path)\n",
    "    gid = str_uuid()\n",
    "    n4j = creat_metagraph(content, gid, n4j)\n",
    "    link_context(n4j, 'str')\n",
    "    \n",
    "merge_similar_nodes(n4j, None)\n",
    "\n",
    "question = load_high(\"./prompt.txt\")\n",
    "sum = process_chunks(question)\n",
    "gid = seq_ret(n4j, sum)\n",
    "response = get_response(n4j, gid, question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
