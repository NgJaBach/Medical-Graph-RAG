{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a57e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain import hub\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from camel.storages import Neo4jGraph\n",
    "from camel.agents import KnowledgeGraphAgent\n",
    "from camel.loaders import UnstructuredIO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import repeat\n",
    "import tiktoken\n",
    "import shortuuid\n",
    "from typing import List\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")\n",
    "\n",
    "n4j = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URL\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = \"gpt-4.1-nano\"\n",
    "embedder_name = \"text-embedding-3-small\"\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def ask_gpt(user, sys) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys},\n",
    "            {\"role\": \"user\", \"content\": f\" {user}\"},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedder_name\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42824eb",
   "metadata": {},
   "source": [
    "# Agentic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e46ae34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(file, message):\n",
    "    with open(file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "\n",
    "def clearing(file):\n",
    "    with open(file, \"w\") as f:\n",
    "        f.write(\"\")\n",
    "\n",
    "class AgenticChunker:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}\n",
    "        self.llm = ChatOpenAI(model=model_name, api_key=openai_api_key, temperature=0)\n",
    "\n",
    "    def add_propositions(self, propositions):\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "    \n",
    "    def add_proposition(self, proposition):\n",
    "        if len(self.chunks) == 0:\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "\n",
    "        chunk_id = self._find_relevant_chunk(proposition)\n",
    "\n",
    "        if chunk_id:\n",
    "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
    "            return\n",
    "        else:\n",
    "            self._create_new_chunk(proposition)\n",
    "        \n",
    "\n",
    "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
    "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
    "        self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
    "        self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
    "\n",
    "    def _update_chunk_summary(self, chunk):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the chunk new summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary']\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _update_chunk_title(self, chunk):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good title will say what the chunk is about.\n",
    "\n",
    "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
    "\n",
    "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        updated_chunk_title = runnable.invoke({\n",
    "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
    "            \"current_summary\" : chunk['summary'],\n",
    "            \"current_title\" : chunk['title']\n",
    "        }).content\n",
    "\n",
    "        return updated_chunk_title\n",
    "\n",
    "    def _get_new_chunk_summary(self, proposition):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
    "\n",
    "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Proposition: Greg likes to eat pizza\n",
    "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "                    Only respond with the new chunk summary, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_summary = runnable.invoke({\n",
    "            \"proposition\": proposition\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_summary\n",
    "    \n",
    "    def _get_new_chunk_title(self, summary):\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "                    A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "                    You will be given a summary of a chunk which needs a title\n",
    "\n",
    "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "                    Or month, generalize it to \"date and times\".\n",
    "\n",
    "                    Example:\n",
    "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
    "                    Output: Date & Times\n",
    "\n",
    "                    Only respond with the new chunk title, nothing else.\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        new_chunk_title = runnable.invoke({\n",
    "            \"summary\": summary\n",
    "        }).content\n",
    "\n",
    "        return new_chunk_title\n",
    "\n",
    "\n",
    "    def _create_new_chunk(self, proposition):\n",
    "        new_chunk_id = str(shortuuid.uuid())\n",
    "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
    "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
    "\n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id' : new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title' : new_chunk_title,\n",
    "            'summary': new_chunk_summary,\n",
    "            'chunk_index' : len(self.chunks)\n",
    "        }\n",
    "    \n",
    "    def get_chunk_outline(self):\n",
    "        \"\"\"\n",
    "        Get a string which represents the chunks you currently have.\n",
    "        This will be empty when you first start off\n",
    "        \"\"\"\n",
    "        chunk_outline = \"\"\n",
    "\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            single_chunk_string = f\"\"\"Chunk ID: {chunk['chunk_id']}\\nChunk Name: {chunk['title']}\\nChunk Summary: {chunk['summary']}\\n\\n\"\"\"\n",
    "            chunk_outline += single_chunk_string\n",
    "        \n",
    "        return chunk_outline\n",
    "\n",
    "    def _find_relevant_chunk(self, proposition):\n",
    "        current_chunk_outline = self.get_chunk_outline()\n",
    "\n",
    "        PROMPT = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
    "\n",
    "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
    "                    The goal is to group similar propositions and chunks.\n",
    "\n",
    "                    If you think a proposition should be joined with a chunk, return the chunk id. Example output: \"k7HRjLxddUJLYtChwqPTqf\"\n",
    "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
    "                (\"user\", \"Proposition:\\n{proposition}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        runnable = PROMPT | self.llm\n",
    "\n",
    "        chunk_found = runnable.invoke({\n",
    "            \"proposition\": proposition,\n",
    "            \"current_chunk_outline\": current_chunk_outline\n",
    "        }).content\n",
    "\n",
    "        if chunk_found in self.chunks:\n",
    "            return chunk_found\n",
    "        elif chunk_found == \"No chunks\":\n",
    "            return None\n",
    "        logging(\"Abnormalities.txt\", f\"---------\\nAbnormality Detected!\\n{current_chunk_outline}\\n\\nChunk got: {chunk_found}\\n\")\n",
    "        return None\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        chunks = []\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
    "        return chunks\n",
    "    \n",
    "    def pretty_print_chunks(self):\n",
    "        clearing(\"current_chunks.txt\")\n",
    "        logging(\"current_chunks.txt\", f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            logging(\"current_chunks.txt\", f\"Chunk #{chunk['chunk_index']}\")\n",
    "            logging(\"current_chunks.txt\", f\"Chunk ID: {chunk_id}\")\n",
    "            logging(\"current_chunks.txt\", f\"Summary: {chunk['summary']}\")\n",
    "            logging(\"current_chunks.txt\", f\"Propositions:\")\n",
    "            for prop in chunk['propositions']:\n",
    "                logging(\"current_chunks.txt\", f\"    -{prop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaaf80",
   "metadata": {},
   "source": [
    "# Data Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cfbdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "def load_high(datapath):\n",
    "    all_content = \"\"  # Initialize an empty string to hold all the content\n",
    "    with open(datapath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            all_content += line.strip() + \"\\n\"  # Append each line to the string, add newline character if needed\n",
    "    return all_content\n",
    "    \n",
    "# Pydantic data class\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "\n",
    "def get_propositions(text, runnable, structured_llm):\n",
    "    runnable_output = runnable.invoke({\n",
    "        \"input\": text\n",
    "    }).content\n",
    "    propositions = structured_llm.invoke(runnable_output)\n",
    "    return propositions.sentences\n",
    "\n",
    "def run_chunk(essay):\n",
    "    obj = hub.pull(\"ahsen/proposal-indexing-zero-shot\")\n",
    "    llm = ChatOpenAI(model=model_name, api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "    runnable = obj | llm\n",
    "\n",
    "    # Extraction\n",
    "    structured_llm = llm.with_structured_output(Sentences)\n",
    "    paragraphs = essay.split(\"\\n\\n\")\n",
    "    essay_propositions = []\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        propositions = get_propositions(para, runnable, structured_llm)\n",
    "        essay_propositions.extend(propositions)\n",
    "\n",
    "    ac = AgenticChunker()\n",
    "    ac.add_propositions(essay_propositions)\n",
    "    ac.pretty_print_chunks()\n",
    "    chunks = ac.get_chunks()\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa71ec",
   "metadata": {},
   "source": [
    "# Summerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd561aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prompt = \"\"\"\n",
    "Generate a structured summary from the provided medical source (report, paper, or book), strictly adhering to the following categories. The summary should list key information under each category in a concise format: 'CATEGORY_NAME: Key information'. No additional explanations or detailed descriptions are necessary unless directly related to the categories:\n",
    "Each category should be addressed only if relevant to the content of the medical source. Ensure the summary is clear and direct, suitable for quick reference.\n",
    "\n",
    "ANATOMICAL_STRUCTURE: Mention any anatomical structures specifically discussed.\n",
    "BODY_FUNCTION: List any body functions highlighted.\n",
    "BODY_MEASUREMENT: Include normal measurements like blood pressure or temperature.\n",
    "BM_RESULT: Results of these measurements.\n",
    "BM_UNIT: Units for each measurement.\n",
    "BM_VALUE: Values of these measurements.\n",
    "LABORATORY_DATA: Outline any laboratory tests mentioned.\n",
    "LAB_RESULT: Outcomes of these tests (e.g., 'increased', 'decreased').\n",
    "LAB_VALUE: Specific values from the tests.\n",
    "LAB_UNIT: Units of measurement for these values.\n",
    "MEDICINE: Name medications discussed.\n",
    "MED_DOSE, MED_DURATION, MED_FORM, MED_FREQUENCY, MED_ROUTE, MED_STATUS, MED_STRENGTH, MED_UNIT, MED_TOTALDOSE: Provide concise details for each medication attribute.\n",
    "PROBLEM: Identify any medical conditions or findings.\n",
    "PROCEDURE: Describe any procedures.\n",
    "PROCEDURE_RESULT: Outcomes of these procedures.\n",
    "PROC_METHOD: Methods used.\n",
    "SEVERITY: Severity of the conditions mentioned.\n",
    "MEDICAL_DEVICE: List any medical devices used.\n",
    "SUBSTANCE_ABUSE: Note any substance abuse mentioned.\n",
    "\"\"\"\n",
    "\n",
    "def split_into_chunks(text, tokens=4000): # split text into chunks of 4000 tokens\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    words = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), tokens):\n",
    "        chunks.append(' '.join(encoding.decode(words[i:i + tokens])))\n",
    "    return chunks   \n",
    "\n",
    "def process_chunks(content: str):\n",
    "    chunks = split_into_chunks(content)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        responses = list(executor.map(ask_gpt, chunks, repeat(sum_prompt)))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca3c3d",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa569f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ge_emb(graph_element):\n",
    "    for node in graph_element.nodes:\n",
    "        emb = get_embedding(node.id)\n",
    "        node.properties['embedding'] = emb\n",
    "    return graph_element\n",
    "\n",
    "def add_gid(graph_element, gid):\n",
    "    for node in graph_element.nodes:\n",
    "        node.properties['gid'] = gid\n",
    "    for rel in graph_element.relationships:\n",
    "        rel.properties['gid'] = gid\n",
    "    return graph_element\n",
    "\n",
    "def add_sum(n4j,content,gid):\n",
    "    sum = process_chunks(content)\n",
    "    creat_sum_query = \"\"\"\n",
    "        CREATE (s:Summary {content: $sum, gid: $gid})\n",
    "        RETURN s\n",
    "        \"\"\"\n",
    "    s = n4j.query(creat_sum_query, {'sum': sum, 'gid': gid})\n",
    "    \n",
    "    link_sum_query = \"\"\"\n",
    "        MATCH (s:Summary {gid: $gid}), (n)\n",
    "        WHERE n.gid = s.gid AND NOT n:Summary\n",
    "        CREATE (s)-[:SUMMARIZES]->(n)\n",
    "        RETURN s, n\n",
    "        \"\"\"\n",
    "    n4j.query(link_sum_query, {'gid': gid})\n",
    "\n",
    "    return s\n",
    "\n",
    "threshold = 0.8\n",
    "def merge_similar_nodes(n4j, gid):\n",
    "    if gid:\n",
    "        merge_query = \"\"\"\n",
    "            WITH $threshold AS threshold\n",
    "            MATCH (n), (m)\n",
    "            WHERE NOT n:Summary AND NOT m:Summary AND n.gid = m.gid AND n.gid = $gid AND n<>m AND apoc.coll.sort(labels(n)) = apoc.coll.sort(labels(m))\n",
    "            WITH n, m,\n",
    "                vector.similarity.cosine(n.embedding, m.embedding) AS similarity\n",
    "            WHERE similarity > threshold\n",
    "            CALL apoc.refactor.mergeNodes([n,m], {properties: 'overwrite', mergeRels: true})\n",
    "            YIELD node\n",
    "            RETURN count(*)\n",
    "        \"\"\"\n",
    "        n4j.query(merge_query, {'gid': gid, 'threshold': threshold})\n",
    "    else:\n",
    "        result = n4j.query(\"\"\"\n",
    "                MATCH (n)\n",
    "                WHERE NOT n:Summary AND n.embedding IS NOT NULL\n",
    "                RETURN elementId(n) AS id, n.embedding AS emb\n",
    "            \"\"\")\n",
    "        nodes = [(record[\"id\"], record[\"emb\"]) for record in result]\n",
    "        n = len(nodes)\n",
    "        total_pairs = n * (n - 1) // 2\n",
    "        for (id1, emb1), (id2, emb2) in tqdm(combinations(nodes, 2), total=total_pairs, desc=\"Merging similar nodes\", unit=\"pairs\"):\n",
    "            a = np.array(emb1, dtype=float)\n",
    "            b = np.array(emb2, dtype=float)\n",
    "            sim = float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "            if sim <= threshold:\n",
    "                continue\n",
    "            n4j.query(\"\"\"\n",
    "                UNWIND $nodeIds AS nid\n",
    "                MATCH (n) WHERE elementId(n)=nid\n",
    "                WITH collect(n) AS nodes\n",
    "                CALL apoc.refactor.mergeNodes(nodes, {properties:'overwrite', mergeRels:true})\n",
    "                YIELD node\n",
    "                RETURN node\n",
    "            \"\"\", {'nodeIds': [id1, id2]})\n",
    "\n",
    "def creat_metagraph(content, gid, n4j):\n",
    "    uio = UnstructuredIO()\n",
    "    kg_agent = KnowledgeGraphAgent()\n",
    "    whole_chunk = content\n",
    "    content = run_chunk(content)\n",
    "    for cont in tqdm(content):\n",
    "        element_example = uio.create_element_from_text(text=cont)\n",
    "        graph_elements = kg_agent.run(element_example, parse_graph_elements=True)\n",
    "        graph_elements = add_ge_emb(graph_elements)\n",
    "        graph_elements = add_gid(graph_elements, gid)\n",
    "        n4j.add_graph_elements(graph_elements=[graph_elements])\n",
    "    \n",
    "    merge_similar_nodes(n4j, gid)\n",
    "    add_sum(n4j, whole_chunk, gid)\n",
    "    return n4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c9af5",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47fd24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_of_largest(nums):\n",
    "    sorted_with_index = sorted((num, index) for index, num in enumerate(nums)) # Sorting the list while keeping track of the original indexes\n",
    "    largest_original_index = sorted_with_index[-1][1] # Extracting the original index of the largest element\n",
    "    \n",
    "    return largest_original_index\n",
    "\n",
    "sys_p = \"\"\"\n",
    "Assess the similarity of the two provided summaries and return a rating from these options: 'very similar', 'similar', 'general', 'not similar', 'totally not similar'. Provide only the rating.\n",
    "\"\"\"\n",
    "\n",
    "def seq_ret(n4j, sumq):\n",
    "    rating_list = []\n",
    "    sumk = []\n",
    "    gids = []\n",
    "    sum_query = \"\"\"\n",
    "        MATCH (s:Summary)\n",
    "        RETURN s.content, s.gid\n",
    "        \"\"\"\n",
    "    res = n4j.query(sum_query)\n",
    "    for r in res:\n",
    "        sumk.append(r['s.content'])\n",
    "        gids.append(r['s.gid'])\n",
    "    \n",
    "    for sk in sumk:\n",
    "        sk = sk[0]\n",
    "        rate = ask_gpt(\"The two summaries for comparison are: \\n Summary 1: \" + sk + \"\\n Summary 2: \" + sumq[0], sys_p)\n",
    "        if \"totally not similar\" in rate:\n",
    "            rating_list.append(0)\n",
    "        elif \"not similar\" in rate:\n",
    "            rating_list.append(1)\n",
    "        elif \"general\" in rate:\n",
    "            rating_list.append(2)\n",
    "        elif \"very similar\" in rate:\n",
    "            rating_list.append(4)\n",
    "        elif \"similar\" in rate:\n",
    "            rating_list.append(3)\n",
    "        else:\n",
    "            print(\"llm returns no relevant rate\")\n",
    "            rating_list.append(-1)\n",
    "\n",
    "    ind = find_index_of_largest(rating_list)\n",
    "    gid = gids[ind]\n",
    "    \n",
    "    return gid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ffb3f",
   "metadata": {},
   "source": [
    "# Clear Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29ad1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Neo4jConnection:\n",
    "#     def __init__(self, uri, user, pwd):\n",
    "#         self.driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "#     def close(self):\n",
    "#         self.driver.close()\n",
    "\n",
    "#     def clean_graph(self):\n",
    "#         with self.driver.session() as session:\n",
    "#             session.execute_write(self._delete_all)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _delete_all(tx):\n",
    "#         tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "# print(\"Cleaning the graph...\")\n",
    "# conn = Neo4jConnection(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "# conn.clean_graph()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c19fe",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5a232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Processing first floor...\") # -------------------------------------------------------\n",
    "# data_path = \"./patients\"\n",
    "# files = [file for file in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, file))]\n",
    "\n",
    "# k = 100\n",
    "# print(f\"Number of patients: {len(files)}\")\n",
    "# print(f\"Randomly selecting {k} patients...\")\n",
    "# k_selected = random.sample(files, k)\n",
    "\n",
    "# with open('selected_patients_list.txt', 'w') as file:\n",
    "#     np.savetxt(file, k_selected, fmt=\"%s\")\n",
    "\n",
    "# clearing(\"Abnormalities.txt\")\n",
    "\n",
    "# for i, file_name in enumerate(k_selected):\n",
    "#     print(f\"Processing {i + 1}th patient...\")\n",
    "#     file_path = os.path.join(data_path, file_name)\n",
    "#     content = load_high(file_path)\n",
    "#     gid = str(shortuuid.uuid()) # Generate a random UUID\n",
    "#     n4j = creat_metagraph(content, gid, n4j)\n",
    "\n",
    "# merge_similar_nodes(n4j, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31fa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing second floor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Processing second floor...\") # -------------------------------------------------------\n",
    "data_path = \"./books_medqa\"\n",
    "files = [file for file in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, file))]\n",
    "for file_name in tqdm(files):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    content = load_high(file_path)\n",
    "    gid = str(shortuuid.uuid()) # Generate a random UUID\n",
    "    n4j = creat_metagraph(content, gid, n4j)\n",
    "merge_similar_nodes(n4j, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bioc import pubtator\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# print(\"Processing third floor...\") # -------------------------------------------------------\n",
    "# with open(\"corpus.txt\", 'r') as fp:\n",
    "#     docs = pubtator.load(fp)\n",
    "\n",
    "# for doc in tqdm(docs):\n",
    "#     content = \"Title: \" + doc.title + \"\\n\" + \"Abstract: \" + doc.abstract\n",
    "#     gid = str(shortuuid.uuid()) # Generate a random UUID\n",
    "#     n4j = creat_metagraph(content, gid, n4j)\n",
    "# merge_similar_nodes(n4j, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def link_context(n4j, gid):\n",
    "#     cont = []\n",
    "#     retrieve_query = \"\"\"\n",
    "#         // Match all 'n' nodes with a specific gid but not of the \"Summary\" type\n",
    "#         MATCH (n)\n",
    "#         WHERE n.gid = $gid AND NOT n:Summary\n",
    "\n",
    "#         // Find all 'm' nodes where 'm' is a reference of 'n' via a 'REFERENCES' relationship\n",
    "#         MATCH (n)-[r:REFERENCE]->(m)\n",
    "#         WHERE NOT m:Summary\n",
    "\n",
    "#         // Find all 'o' nodes connected to each 'm', and include the relationship type,\n",
    "#         // while excluding 'Summary' type nodes and 'REFERENCE' relationship\n",
    "#         MATCH (m)-[s]-(o)\n",
    "#         WHERE NOT o:Summary AND TYPE(s) <> 'REFERENCE'\n",
    "\n",
    "#         // Collect and return details in a structured format\n",
    "#         RETURN n.id AS NodeId1, \n",
    "#             m.id AS Mid, \n",
    "#             TYPE(r) AS ReferenceType, \n",
    "#             collect(DISTINCT {RelationType: type(s), Oid: o.id}) AS Connections\n",
    "#     \"\"\"\n",
    "#     res = n4j.query(retrieve_query, {'gid': gid})\n",
    "#     for r in res:\n",
    "#         # Expand each set of connections into separate entries with n and m\n",
    "#         for ind, connection in enumerate(r[\"Connections\"]):\n",
    "#             cont.append(\"Reference \" + str(ind) + \": \" + r[\"NodeId1\"] + \"has the reference that\" + r['Mid'] + connection['RelationType'] + connection['Oid'])\n",
    "#     return cont\n",
    "\n",
    "# def ret_context(n4j, gid):\n",
    "#     cont = []\n",
    "#     ret_query = \"\"\"\n",
    "#     // Match all nodes with a specific gid but not of type \"Summary\" and collect them\n",
    "#     MATCH (n)\n",
    "#     WHERE n.gid = $gid AND NOT n:Summary\n",
    "#     WITH collect(n) AS nodes\n",
    "\n",
    "#     // Unwind the nodes to a pairs and match relationships between them\n",
    "#     UNWIND nodes AS n\n",
    "#     UNWIND nodes AS m\n",
    "#     MATCH (n)-[r]-(m)\n",
    "#     WHERE n.gid = m.gid AND id(n) < id(m) AND NOT n:Summary AND NOT m:Summary // Ensure each pair is processed once and exclude \"Summary\" nodes in relationships\n",
    "#     WITH n, m, TYPE(r) AS relType\n",
    "\n",
    "#     // Return node IDs and relationship types in structured format\n",
    "#     RETURN n.id AS NodeId1, relType, m.id AS NodeId2\n",
    "#     \"\"\"\n",
    "#     res = n4j.query(ret_query, {'gid': gid})\n",
    "#     for r in res:\n",
    "#         cont.append(r['NodeId1'] + r['relType'] + r['NodeId2'])\n",
    "#     return cont\n",
    "\n",
    "# sys_prompt_zero = \"\"\"\n",
    "# Please answer the question based on the provided information and references.\n",
    "# There are 4 options for the answer, strictly choose one of them and say nothing else.\n",
    "# Example output: \"A\"\n",
    "# \"\"\"\n",
    "\n",
    "# def get_response(n4j, gid, query):\n",
    "#     selfcont = ret_context(n4j, gid)\n",
    "#     linkcont = link_context(n4j, gid)\n",
    "#     user_zero = f\"\"\"\n",
    "#     The question is: {query}\n",
    "#     The provided information is: {selfcont}\n",
    "#     The references are: {linkcont}\n",
    "#     \"\"\"\n",
    "#     res = ask_gpt(user_zero, sys_prompt_zero)\n",
    "#     return res\n",
    "\n",
    "# question = load_high(\"example.txt\")\n",
    "# sum = process_chunks(question)\n",
    "# gid = seq_ret(n4j, sum)\n",
    "# response = get_response(n4j, gid, question)\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
